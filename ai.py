# server_webrtc.py - AWS EC2ìš© WebRTC ê¸°ë°˜ ë©´ì ‘ ë¶„ì„ ì‹œìŠ¤í…œ
# í´ë¼ì´ì–¸íŠ¸ ë¸Œë¼ìš°ì €ì—ì„œ ì¹´ë©”ë¼ ì˜ìƒì„ ë°›ì•„ì„œ ë¶„ì„

from flask import Flask, jsonify, request
from flask_cors import CORS
import mediapipe as mp
import numpy as np
import cv2
import json
import time
from collections import deque, Counter
import threading
import random
import atexit
import io
import os
import base64

# AI ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from pydub import AudioSegment
    AUDIO_PROCESSING_AVAILABLE = True
    print("âœ… pydub ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")
except ImportError:
    AUDIO_PROCESSING_AVAILABLE = False
    print("âš ï¸ pydub ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

try:
    import whisper
    WHISPER_AVAILABLE = True
    print("âœ… Whisper ë¡œë“œ ì™„ë£Œ")
except ImportError:
    WHISPER_AVAILABLE = False
    print("âš ï¸ Whisperê°€ ì—†ìŠµë‹ˆë‹¤. pip install openai-whisper")

try:
    from konlpy.tag import Okt
    KONLPY_AVAILABLE = False
    print("âœ… KoNLPy ë¡œë“œ ì™„ë£Œ")
except ImportError:
    KONLPY_AVAILABLE = False
    print("âš ï¸ KoNLPyê°€ ì—†ìŠµë‹ˆë‹¤. pip install konlpy")

try:
    import librosa
    import soundfile as sf
    LIBROSA_AVAILABLE = True
    print("âœ… librosa ë¡œë“œ ì™„ë£Œ")
except ImportError:
    LIBROSA_AVAILABLE = False
    print("âš ï¸ librosaê°€ ì—†ìŠµë‹ˆë‹¤. pip install librosa soundfile")

app = Flask(__name__)
CORS(app)

mp_pose = mp.solutions.pose

# MediaPipe ì´ˆê¸°í™”
try:
    pose = mp_pose.Pose(
        static_image_mode=False,
        model_complexity=1,
        smooth_landmarks=True,
        enable_segmentation=False,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    )
    print("âœ… MediaPipe Pose ì´ˆê¸°í™” ì™„ë£Œ")
except Exception as e:
    print(f"âŒ MediaPipe Pose ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    pose = None

# AI ëª¨ë¸ ì´ˆê¸°í™”
whisper_model = None
okt = None

if WHISPER_AVAILABLE:
    print("ğŸ”„ Whisper ëª¨ë¸ ë¡œë”© ì¤‘...")
    whisper_model = whisper.load_model("tiny")
    print("âœ… Whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

if KONLPY_AVAILABLE:
    okt = Okt()
    print("âœ… KoNLPy Okt ì´ˆê¸°í™” ì™„ë£Œ")

# ê¸°ì¡´ ì½”ë“œì˜ ëª¨ë“  ì „ì—­ ë³€ìˆ˜ë“¤ (voice_analysis_data, FILLER_WORDS, INTERVIEW_QUESTIONS ë“±)
voice_analysis_data = {
    "session_active": False,
    "start_time": None,
    "audio_samples": [],
    "filler_words": [],
    "voice_confidence": [],
    "speaking_pace": [],
    "volume_levels": [],
    "total_speaking_time": 0,
    "silence_periods": [],
    "analysis_complete": False,
    "final_report": {},
    "chunk_count": 0,
    "ai_analysis": {}
}

FILLER_WORDS = [
    "ì–´", "ìŒ", "ì•„", "ê·¸", "ë­", "ì´ì œ", "ê·¸ë˜ì„œ", "ê·¸ë‹ˆê¹Œ", "ê·¸ëŸ°ë°",
    "ì–´ë–»ê²Œ", "ë­”ê°€", "ì•½ê°„", "ì¢€", "ê·¸ê±°", "ì´ê±°", "ì €ê¸°", "ì•„ë¬´íŠ¼",
    "ì¼ë‹¨", "ìš°ì„ ", "ê·¸ëŸ¬ë©´", "ê·¸ëŸ¼", "ì•„ë‹ˆ", "ë§ì•„", "ê·¸ì¹˜", "ì‘", "ë„¤",
    "ì—", "ì—„", "í ", "ì•„ë¬´ë˜ë„"
]

CONNECTIVES = [
    "ê·¸ë˜ì„œ", "ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ê·¸ë¦¬ê³ ", "ë˜í•œ", "ë”°ë¼ì„œ", "ê·¸ëŸ°ë°",
    "ì™œëƒí•˜ë©´", "ì¦‰", "ê²°êµ­", "ê·¸ëŸ¬ë¯€ë¡œ", "ê·¸ë ‡ì§€ë§Œ", "ë˜ëŠ”", "í˜¹ì€",
    "ê·¸ëŸ¼ì—ë„", "ë°˜ë©´ì—", "í•œí¸", "ë”ë¶ˆì–´"
]

INTERVIEW_QUESTIONS = {
    "ê¸°ë³¸ì§ˆë¬¸": [
        "ê°„ë‹¨í•˜ê²Œ ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.",
        "ì²´ìœ¡í•™ê³¼ì— ì§€ì›í•œ ë™ê¸°ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ë³¸ì¸ì˜ ì¥ì ê³¼ ë‹¨ì ì„ ì´ì•¼ê¸°í•´ ì£¼ì„¸ìš”.",
        "ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ìš´ë™ ì¢…ëª©ì€ ë¬´ì—‡ì´ê³ , ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ëŒ€í•™êµ 4ë…„ ë™ì•ˆ ë¬´ì—‡ì„ ë°°ìš°ê³  ì‹¶ë‚˜ìš”?"
    ],
    "ì§€ì›ë™ê¸°": [
        "ìš°ë¦¬ í•™êµ ì²´ìœ¡í•™ê³¼ë¥¼ ì„ íƒí•œ íŠ¹ë³„í•œ ì´ìœ ê°€ ìˆë‚˜ìš”?",
        "ì²´ìœ¡êµì‚¬ë‚˜ ìŠ¤í¬ì¸ ì§€ë„ìê°€ ë˜ê³  ì‹¶ì€ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ì¡¸ì—… í›„ ì§„ë¡œ ê³„íšì— ëŒ€í•´ êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ì£¼ì„¸ìš”.",
        "ì²´ìœ¡ ë¶„ì•¼ì—ì„œ ë³¸ì¸ë§Œì˜ ëª©í‘œë‚˜ ë¹„ì „ì´ ìˆë‹¤ë©´ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì´ ë¶„ì•¼ë¥¼ ì„ íƒí•˜ê²Œ ëœ ê³„ê¸°ë‚˜ ì˜í–¥ì„ ë°›ì€ ì‚¬ëŒì´ ìˆë‚˜ìš”?"
    ],
    "ìœ¡ìƒì „ë¬¸": [
        "ìœ¡ìƒ ìš´ë™ì„ ì‹œì‘í•˜ê²Œ ëœ ê³„ê¸°ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ë³¸ì¸ì´ ê°€ì¥ ìì‹  ìˆëŠ” ìœ¡ìƒ ì¢…ëª©ì€ ë¬´ì—‡ì´ê³ , ì–´ë–¤ ê¸°ë¡ì„ ê°€ì§€ê³  ìˆë‚˜ìš”?",
        "ìœ¡ìƒ ìš´ë™ì˜ ë§¤ë ¥ì€ ë¬´ì—‡ì´ë¼ê³  ìƒê°í•˜ë‚˜ìš”?",
        "ë‹¨ê±°ë¦¬ì™€ ì¥ê±°ë¦¬ ìœ¡ìƒì˜ ì°¨ì´ì ì— ëŒ€í•´ ì„¤ëª…í•´ë³´ì„¸ìš”.",
        "ìœ¡ìƒ ì„ ìˆ˜ì—ê²Œ í•„ìš”í•œ ì²´ë ¥ ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ì¢‹ì•„í•˜ëŠ” ìœ¡ìƒ ì„ ìˆ˜ê°€ ìˆë‹¤ë©´ ëˆ„êµ¬ì´ê³  ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
    ],
    "ìƒí™©ì§ˆë¬¸": [
        "íŒ€ ë‚´ì—ì„œ ê°ˆë“±ì´ ìƒê²¼ì„ ë•Œ ì–´ë–»ê²Œ í•´ê²°í•˜ì‹œê² ë‚˜ìš”?",
        "ì¤‘ìš”í•œ ëŒ€íšŒë¥¼ ì•ë‘ê³  ë¶€ìƒì„ ë‹¹í–ˆë‹¤ë©´ ì–´ë–»ê²Œ í•˜ì‹œê² ë‚˜ìš”?",
        "í›ˆë ¨ì´ í˜ë“¤ì–´ì„œ í¬ê¸°í•˜ê³  ì‹¶ì„ ë•ŒëŠ” ì–´ë–»ê²Œ ê·¹ë³µí•˜ë‚˜ìš”?",
        "ëª©í‘œë¥¼ ë‹¬ì„±í•˜ì§€ ëª»í–ˆì„ ë•Œì˜ ê²½í—˜ê³¼ ê·¹ë³µ ë°©ë²•ì„ ë§í•´ë³´ì„¸ìš”.",
        "í›„ë°°ë‚˜ ë™ë£Œì™€ ì˜ê²¬ ì°¨ì´ê°€ ìˆì„ ë•Œ ì–´ë–»ê²Œ í•´ê²°í•˜ë‚˜ìš”?",
        "ì‹¤ìˆ˜ë¥¼ í–ˆì„ ë•Œ ì–´ë–»ê²Œ ëŒ€ì²˜í•˜ë‚˜ìš”?"
    ],
    "ì••ë°•ì§ˆë¬¸": [
        "ë‹¤ë¥¸ ì§€ì›ìë“¤ê³¼ ë¹„êµí–ˆì„ ë•Œ ë³¸ì¸ì˜ ê²½ìŸë ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì²´ìœ¡ ì„±ì ì´ ì¢‹ì§€ ì•Šë‹¤ë©´ ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ìš´ë™ì„ ì‹«ì–´í•˜ëŠ” í•™ìƒì—ê²Œ ì–´ë–»ê²Œ ë™ê¸°ë¶€ì—¬ë¥¼ í•  ê²ƒì¸ê°€ìš”?",
        "ì²´ìœ¡ ìˆ˜ì—… ì‹œê°„ì— ë‹¤ì¹œ í•™ìƒì´ ìˆë‹¤ë©´ ì–´ë–»ê²Œ ëŒ€ì²˜í•˜ì‹œê² ë‚˜ìš”?",
        "ë³¸ì¸ì˜ ê°€ì¥ í° ì•½ì ì€ ë¬´ì—‡ì´ê³ , ì–´ë–»ê²Œ ê°œì„ í•˜ê³  ìˆë‚˜ìš”?",
        "ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ì„ ë•Œ ì–´ë–»ê²Œ í•´ì†Œí•˜ì‹œë‚˜ìš”?"
    ],
    "ë§ˆë¬´ë¦¬ì§ˆë¬¸": [
        "ìš°ë¦¬ í•™êµì— ì…í•™í•˜ë©´ ê°€ì¥ ë¨¼ì € í•˜ê³  ì‹¶ì€ ì¼ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "10ë…„ í›„ ë³¸ì¸ì˜ ëª¨ìŠµì„ ì–´ë–»ê²Œ ê·¸ë ¤ë³´ê³  ìˆë‚˜ìš”?",
        "ì²´ìœ¡ ì§€ë„ìë¡œì„œ ê°€ì¥ ë³´ëŒ ìˆì„ ê²ƒ ê°™ì€ ìˆœê°„ì€ ì–¸ì œì¸ê°€ìš”?",
        "ë§ˆì§€ë§‰ìœ¼ë¡œ í•˜ê³  ì‹¶ì€ ë§ì”€ì´ ìˆë‹¤ë©´ í•´ì£¼ì„¸ìš”.",
        "ìš°ë¦¬ì—ê²Œ ê¼­ ë¬¼ì–´ë³´ê³  ì‹¶ì€ ê²ƒì´ ìˆë‚˜ìš”?"
    ]
}

INTERVIEW_FLOW = [
    "ê¸°ë³¸ì§ˆë¬¸", "ì§€ì›ë™ê¸°", "ìœ¡ìƒì „ë¬¸", "ìƒí™©ì§ˆë¬¸", "ì••ë°•ì§ˆë¬¸", "ë§ˆë¬´ë¦¬ì§ˆë¬¸"
]

interview_session = {
    "active": False,
    "current_question": None,
    "question_index": 0,
    "category": None,
    "questions_list": [],
    "start_time": None,
    "thinking_time": 10,
    "answer_time": 50,
    "phase": "thinking",
    "auto_timer": None,
    "phase_start_time": None,
    "current_flow_stage": 0
}

# ìì„¸ ë¶„ì„ê¸° (ê¸°ì¡´ ì½”ë“œ)
class PrecisePostureAnalyzer:
    def __init__(self):
        self.guide_frame = {
            'x_min': 0.25, 'x_max': 0.75,
            'y_min': 0.15, 'y_max': 0.85
        }
        self.calibrated = False
        self.calibration_samples = []
        self.thresholds = {
            'head_rotation': 0.03, 'head_tilt': 0.02,
            'shoulder_level': 0.05, 'movement': 0.03,
            'face_touch': 0.12, 'arm_cross': 0.15, 'slouch': 0.13
        }
        self.previous_positions = deque(maxlen=15)
        self.last_beep_time = 0
        self.beep_cooldown = 2.0

    def add_calibration_sample(self, landmarks):
        nose = landmarks[mp_pose.PoseLandmark.NOSE.value]
        left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]
        right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]
        shoulder_center_y = (left_shoulder.y + right_shoulder.y) / 2
        nose_to_shoulder_y = nose.y - shoulder_center_y
        self.calibration_samples.append({
            'nose_to_shoulder_y': nose_to_shoulder_y,
            'nose_x': nose.x
        })

    def finalize_calibration(self):
        if len(self.calibration_samples) >= 10:
            avg_nose_to_shoulder_y = sum(s['nose_to_shoulder_y'] for s in self.calibration_samples) / len(self.calibration_samples)
            avg_nose_x = sum(s['nose_x'] for s in self.calibration_samples) / len(self.calibration_samples)
            self.baseline_nose_to_shoulder_y = avg_nose_to_shoulder_y
            self.baseline_nose_x = avg_nose_x
            self.calibrated = True
            print(f"âœ… ìì„¸ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì™„ë£Œ!")
            return True
        return False

    def reset_calibration(self):
        self.calibrated = False
        self.calibration_samples = []

    def calculate_distance(self, point1, point2):
        return np.sqrt((point1.x - point2.x) ** 2 + (point1.y - point2.y) ** 2)

    def check_frame_position(self, landmarks):
        nose = landmarks[mp_pose.PoseLandmark.NOSE.value]
        margin = 0.1
        x_min_with_margin = self.guide_frame['x_min'] + margin
        x_max_with_margin = self.guide_frame['x_max'] - margin
        y_min_with_margin = self.guide_frame['y_min'] + margin
        y_max_with_margin = self.guide_frame['y_max'] - margin

        out_of_frame = (
            nose.x < x_min_with_margin or nose.x > x_max_with_margin or
            nose.y < y_min_with_margin or nose.y > y_max_with_margin
        )

        return not out_of_frame

    def analyze_face_direction(self, landmarks):
        issues = []
        left_eye = landmarks[mp_pose.PoseLandmark.LEFT_EYE.value]
        right_eye = landmarks[mp_pose.PoseLandmark.RIGHT_EYE.value]
        nose = landmarks[mp_pose.PoseLandmark.NOSE.value]

        eye_distance = abs(left_eye.x - right_eye.x)
        eye_height_diff = left_eye.y - right_eye.y
        
        normalized_tilt = eye_height_diff / eye_distance if eye_distance >= 0.05 else 0
        
        self.previous_positions.append({
            'nose_x': nose.x, 'nose_y': nose.y,
            'tilt': normalized_tilt, 'timestamp': time.time()
        })

        if len(self.previous_positions) >= 5:
            recent_tilts = [p['tilt'] for p in list(self.previous_positions)[-5:]]
            avg_tilt = np.mean(recent_tilts)
            
            if avg_tilt > 0.25:
                issues.append("ê³ ê°œê°€ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê¸°ìš¸ì–´ì ¸ ìˆìŠµë‹ˆë‹¤")
            elif avg_tilt < -0.25:
                issues.append("ê³ ê°œê°€ ì™¼ìª½ìœ¼ë¡œ ê¸°ìš¸ì–´ì ¸ ìˆìŠµë‹ˆë‹¤")

        return issues, {'eye_tilt': float(normalized_tilt), 'face_rotation': float(nose.x)}

    def analyze_shoulder_posture(self, landmarks):
        issues = []
        left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]
        right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]
        left_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]
        right_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]

        shoulder_diff = left_shoulder.y - right_shoulder.y
        if shoulder_diff > 0.07:
            issues.append("ì™¼ìª½ ì–´ê¹¨ê°€ ì˜¬ë¼ê°€ ìˆìŠµë‹ˆë‹¤")
        elif shoulder_diff < -0.07:
            issues.append("ì˜¤ë¥¸ìª½ ì–´ê¹¨ê°€ ì˜¬ë¼ê°€ ìˆìŠµë‹ˆë‹¤")

        shoulder_center_y = (left_shoulder.y + right_shoulder.y) / 2
        hip_center_y = (left_hip.y + right_hip.y) / 2
        if shoulder_center_y > hip_center_y + 0.13:
            issues.append("ë“±ì„ í´ê³  ë°”ë¥´ê²Œ ì•‰ìœ¼ì„¸ìš”")

        return issues

    def analyze_hand_gestures(self, landmarks):
        issues = []
        nose = landmarks[mp_pose.PoseLandmark.NOSE.value]
        left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]
        right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]

        if self.calculate_distance(left_wrist, nose) < self.thresholds['face_touch']:
            issues.append("ì™¼ì†ì´ ì–¼êµ´ ê·¼ì²˜ì— ìˆìŠµë‹ˆë‹¤")
        if self.calculate_distance(right_wrist, nose) < self.thresholds['face_touch']:
            issues.append("ì˜¤ë¥¸ì†ì´ ì–¼êµ´ ê·¼ì²˜ì— ìˆìŠµë‹ˆë‹¤")

        return issues

    def analyze_movement(self):
        if len(self.previous_positions) < 2:
            return [], 0.0

        recent_positions = list(self.previous_positions)[-5:]
        if len(recent_positions) < 2:
            return [], 0.0

        movements = []
        for i in range(1, len(recent_positions)):
            prev = recent_positions[i-1]
            curr = recent_positions[i]
            movement = np.sqrt(
                (curr['nose_x'] - prev['nose_x']) ** 2 +
                (curr['nose_y'] - prev['nose_y']) ** 2
            )
            movements.append(movement)

        avg_movement = np.mean(movements) if movements else 0.0
        issues = []
        if avg_movement > self.thresholds['movement']:
            issues.append("ëª¸ì´ ë§ì´ í”ë“¤ë¦½ë‹ˆë‹¤")

        return issues, float(avg_movement)

    def analyze_posture(self, landmarks):
        issues = []
        score = 100

        in_frame = self.check_frame_position(landmarks)
        if not in_frame:
            issues.append("ê°€ì´ë“œ ì˜ì—­ ì•ˆìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”")
            score -= 30

        face_issues, _ = self.analyze_face_direction(landmarks)
        issues.extend(face_issues)
        score -= len(face_issues) * 15

        shoulder_issues = self.analyze_shoulder_posture(landmarks)
        issues.extend(shoulder_issues)
        score -= len(shoulder_issues) * 15

        hand_issues = self.analyze_hand_gestures(landmarks)
        issues.extend(hand_issues)
        score -= len(hand_issues) * 10

        movement_issues, movement_val = self.analyze_movement()
        issues.extend(movement_issues)
        score -= len(movement_issues) * 10

        return {
            'score': max(0, score),
            'issues': issues,
            'in_frame': in_frame,
            'movement': movement_val
        }

analyzer = PrecisePostureAnalyzer()

latest_analysis_data = {
    'score': 100, 'issues': [], 'timestamp': time.time(),
    'interview': {
        'active': False, 'current_question': None,
        'phase': 'thinking', 'time_remaining': 0
    }
}

is_analysis_active = False
calibration_in_progress = False

# AI ë¶„ì„ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ)
def analyze_voice_tremor_librosa(audio_path):
    if not LIBROSA_AVAILABLE:
        return None
    
    try:
        y, sr = librosa.load(audio_path, sr=None)
        if len(y) < sr * 0.5:
            return None
        
        pitches, magnitudes = librosa.piptrack(y=y, sr=sr, fmin=75, fmax=300)
        pitch_values = []
        
        for t in range(pitches.shape[1]):
            index = magnitudes[:, t].argmax()
            pitch = pitches[index, t]
            if pitch > 0:
                pitch_values.append(pitch)
        
        if len(pitch_values) > 1:
            pitch_changes = np.diff(pitch_values)
            jitter = np.std(pitch_changes)
            jitter_percent = (jitter / np.mean(pitch_values)) * 100
        else:
            jitter = 0
            jitter_percent = 0
        
        rms = librosa.feature.rms(y=y)[0]
        shimmer = np.std(rms)
        
        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
        spectral_stability = np.std(spectral_centroid)
        
        if jitter_percent < 1.0 and shimmer < 0.02:
            confidence = 95
        elif jitter_percent < 2.0 and shimmer < 0.04:
            confidence = 80
        elif jitter_percent < 3.5 and shimmer < 0.06:
            confidence = 60
        else:
            confidence = 40
        
        result = {
            'jitter': float(jitter),
            'jitter_percent': float(jitter_percent),
            'shimmer': float(shimmer),
            'spectral_stability': float(spectral_stability),
            'average_pitch': float(np.mean(pitch_values)) if pitch_values else 0,
            'pitch_range': float(np.max(pitch_values) - np.min(pitch_values)) if len(pitch_values) > 1 else 0,
            'voice_confidence': confidence
        }
        
        return result
        
    except Exception as e:
        print(f"âŒ librosa ë¶„ì„ ì‹¤íŒ¨: {e}")
        return None

def analyze_speech_with_whisper_konlpy(audio_path):
    if not WHISPER_AVAILABLE:
        return None
    
    try:
        result = whisper_model.transcribe(audio_path, language='ko')
        full_text = result['text']
        
        if not full_text.strip():
            return None
        
        words = full_text.split()
        
        filler_count = Counter()
        for word in words:
            clean_word = word.strip('.,!?').lower()
            if clean_word in FILLER_WORDS:
                filler_count[clean_word] += 1
        
        connective_count = Counter()
        for word in words:
            clean_word = word.strip('.,!?').lower()
            if clean_word in CONNECTIVES:
                connective_count[clean_word] += 1
        
        nouns = []
        verbs = []
        adjectives = []
        adverbs = []
        
        if KONLPY_AVAILABLE and okt:
            try:
                morphs = okt.pos(full_text)
                nouns = [word for word, pos in morphs if pos == 'Noun']
                verbs = [word for word, pos in morphs if pos == 'Verb']
                adjectives = [word for word, pos in morphs if pos == 'Adjective']
                adverbs = [word for word, pos in morphs if pos == 'Adverb']
            except:
                pass
        
        word_freq = Counter(word.strip('.,!?').lower() for word in words if len(word) > 1)
        repeated_words = {word: count for word, count in word_freq.items() if count > 2}
        
        sentences = [s.strip() for s in full_text.split('.') if s.strip()]
        avg_sentence_length = len(words) / len(sentences) if sentences else 0
        
        vocabulary_richness = len(set(nouns)) / len(nouns) if nouns else 0
        
        analysis_result = {
            'full_text': full_text,
            'total_words': len(words),
            'filler_words': dict(filler_count),
            'filler_total': sum(filler_count.values()),
            'filler_ratio': (sum(filler_count.values()) / len(words) * 100) if words else 0,
            'connectives': dict(connective_count),
            'connective_total': sum(connective_count.values()),
            'noun_count': len(nouns),
            'verb_count': len(verbs),
            'adjective_count': len(adjectives),
            'adverb_count': len(adverbs),
            'top_nouns': dict(Counter(nouns).most_common(10)),
            'repeated_words': repeated_words,
            'sentence_count': len(sentences),
            'avg_sentence_length': avg_sentence_length,
            'vocabulary_richness': vocabulary_richness * 100,
            'segments': result.get('segments', [])
        }
        
        return analysis_result
        
    except Exception as e:
        print(f"âŒ Whisper+KoNLPy ë¶„ì„ ì‹¤íŒ¨: {e}")
        return None

def generate_ai_recommendations(tremor, speech):
    recommendations = []
    
    if tremor:
        if tremor['jitter_percent'] > 3.0:
            recommendations.append(f"ëª©ì†Œë¦¬ì— ë–¨ë¦¼ì´ ìˆìŠµë‹ˆë‹¤ (Jitter: {tremor['jitter_percent']:.1f}%). ì‹¬í˜¸í¡ì„ í•˜ê³  ì²œì²œíˆ ë§í•´ë³´ì„¸ìš”.")
        elif tremor['jitter_percent'] > 2.0:
            recommendations.append("ì•½ê°„ì˜ ê¸´ì¥ì´ ëŠê»´ì§‘ë‹ˆë‹¤. í¸ì•ˆí•˜ê²Œ ë‹µë³€í•´ë³´ì„¸ìš”.")
        
        if tremor['shimmer'] > 0.05:
            recommendations.append("ìŒëŸ‰ ë³€í™”ê°€ í½ë‹ˆë‹¤. ì¼ì •í•œ í¬ê¸°ë¡œ ë§í•´ë³´ì„¸ìš”.")
    
    if speech:
        if speech['filler_ratio'] > 10:
            recommendations.append(f"ì¶”ì„ìƒˆê°€ ë§ìŠµë‹ˆë‹¤ ({speech['filler_total']}íšŒ, {speech['filler_ratio']:.1f}%). ì˜ì‹ì ìœ¼ë¡œ ì¤„ì—¬ë³´ì„¸ìš”.")
        
        if speech['connective_total'] > 15:
            recommendations.append(f"ì ‘ì†ì‚¬ ì‚¬ìš©ì´ ë§ìŠµë‹ˆë‹¤ ({speech['connective_total']}íšŒ). ë¬¸ì¥ì„ ê°„ê²°í•˜ê²Œ ë§Œë“¤ì–´ë³´ì„¸ìš”.")
        
        if speech.get('repeated_words'):
            top_repeated = sorted(speech['repeated_words'].items(), key=lambda x: x[1], reverse=True)[:3]
            if top_repeated:
                recommendations.append(f"ë°˜ë³µëœ ë‹¨ì–´: {', '.join([f'{w}({c}íšŒ)' for w, c in top_repeated])}")
        
        if speech['vocabulary_richness'] < 30:
            recommendations.append("ë‹¤ì–‘í•œ ì–´íœ˜ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
    
    if not recommendations:
        recommendations.append("í›Œë¥­í•œ ë‹µë³€ì…ë‹ˆë‹¤!")
    
    return recommendations

def generate_voice_analysis_report():
    """ìŒì„± ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± - íƒ€ì„ìŠ¤íƒ¬í”„ í•„ìˆ˜ í¬í•¨"""
    try:
        print("=" * 70)
        print("ğŸ“Š ìŒì„± ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
        print("=" * 70)
        
        sample_count = len(voice_analysis_data["audio_samples"])
        print(f"ğŸ“¦ ë¶„ì„ ìƒ˜í”Œ: {sample_count}ê°œ")
        print(f"ğŸ“¦ ìˆ˜ì‹  ì²­í¬: {voice_analysis_data.get('chunk_count', 0)}ê°œ")
        
        # ê¸°ë³¸ ìŒì„± ë¶„ì„
        volumes = [sample['volume'] for sample in voice_analysis_data["audio_samples"] if sample]
        speaking_periods = [sample for sample in voice_analysis_data["audio_samples"] 
                          if sample and sample.get('is_speaking', False)]
        
        total_time = time.time() - voice_analysis_data["start_time"] if voice_analysis_data["start_time"] else 0
        speaking_time = len(speaking_periods) * 0.1
        speaking_ratio = speaking_time / total_time if total_time > 0 else 0
        
        avg_volume = np.mean(volumes) if volumes else 0
        volume_std = np.std(volumes) if len(volumes) > 1 else 0
        volume_consistency = max(0, 1 - (volume_std / (avg_volume + 0.001)))
        
        # ê¸°ë³¸ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        confidence_score = (
            (min(avg_volume * 100, 100) * 0.30) +
            (volume_consistency * 100 * 0.25) +
            (speaking_ratio * 100 * 0.20)
        )
        
        print(f"ğŸ“Š ê¸°ë³¸ ë¶„ì„ ê²°ê³¼:")
        print(f"   - í‰ê·  ë³¼ë¥¨: {avg_volume * 100:.1f}%")
        print(f"   - ë³¼ë¥¨ ì¼ê´€ì„±: {volume_consistency * 100:.1f}%")
        print(f"   - ë§í•˜ê¸° ë¹„ìœ¨: {speaking_ratio * 100:.1f}%")
        print(f"   - ê¸°ë³¸ ì‹ ë¢°ë„: {confidence_score:.1f}")
        
        # AI ë¶„ì„ ê²°ê³¼ ë°˜ì˜
        ai_analysis = voice_analysis_data.get("ai_analysis", {})
        
        if ai_analysis:
            print("ğŸ¤– AI ë¶„ì„ ê²°ê³¼ ë°˜ì˜ ì¤‘...")
            
            if ai_analysis.get('tremor'):
                tremor_confidence = ai_analysis['tremor']['voice_confidence']
                confidence_score = (confidence_score * 0.5) + (tremor_confidence * 0.5)
                print(f"   âœ… librosa ë¶„ì„: Jitter={ai_analysis['tremor']['jitter_percent']:.2f}%")
            
            if ai_analysis.get('speech'):
                filler_penalty = min(ai_analysis['speech']['filler_ratio'], 20)
                confidence_score = confidence_score * (1 - filler_penalty / 100)
                print(f"   âœ… Whisper ë¶„ì„: ì¶”ì„ìƒˆ={ai_analysis['speech']['filler_total']}íšŒ")
        
        # ì¶”ì²œì‚¬í•­ ìƒì„±
        recommendations = []
        
        if avg_volume < 0.3:
            recommendations.append("ëª©ì†Œë¦¬ê°€ ì‘ìŠµë‹ˆë‹¤. ë” ìì‹ ìˆê²Œ ë§í•´ë³´ì„¸ìš”.")
        
        if ai_analysis:
            ai_recommendations = generate_ai_recommendations(
                ai_analysis.get('tremor'),
                ai_analysis.get('speech')
            )
            recommendations.extend(ai_recommendations)
        
        if not recommendations:
            recommendations.append("ì „ë°˜ì ìœ¼ë¡œ ì•ˆì •ì ì¸ ìŒì„±ìœ¼ë¡œ ë‹µë³€í•˜ì…¨ìŠµë‹ˆë‹¤!")
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (í•„ìˆ˜!)
        current_timestamp = time.time()
        
        # ë¦¬í¬íŠ¸ êµ¬ì„±
        report = {
            "overall_score": round(confidence_score, 1),
            "analysis_timestamp": current_timestamp,  # í•„ìˆ˜!
            "detailed_analysis": {
                "voice_confidence": round(confidence_score, 1),
                "average_volume": round(avg_volume * 100, 1),
                "volume_consistency": round(volume_consistency * 100, 1),
                "voice_stability": round(volume_consistency * 100, 1),
                "speaking_ratio": round(speaking_ratio * 100, 1),
                "total_speaking_time": round(speaking_time, 1),
                "filler_word_count": 0,
                "filler_ratio": 0
            },
            "recommendations": recommendations[:10],
            "ai_powered": bool(ai_analysis),
            "debug_info": {
                "samples_analyzed": sample_count,
                "chunks_received": voice_analysis_data.get("chunk_count", 0),
                "has_whisper": WHISPER_AVAILABLE,
                "has_librosa": LIBROSA_AVAILABLE
            }
        }

        # AI ì„¸ë¶€ ì •ë³´ ì¶”ê°€
        if ai_analysis.get('speech'):
            speech = ai_analysis['speech']
            report["detailed_analysis"].update({
                "filler_word_count": speech['filler_total'],
                "filler_ratio": round(speech['filler_ratio'], 1),
                "connective_count": speech['connective_total'],
                "vocabulary_richness": round(speech['vocabulary_richness'], 1),
                "avg_sentence_length": speech['avg_sentence_length']
            })
            
            report["ai_details"] = {
                "recognized_text": speech['full_text'][:500],
                "filler_words": speech['filler_words'],
                "connectives": speech['connectives'],
                "top_nouns": speech['top_nouns']
            }

        if ai_analysis.get('tremor'):
            tremor = ai_analysis['tremor']
            report["detailed_analysis"].update({
                "jitter_percent": round(tremor['jitter_percent'], 2),
                "average_pitch": round(tremor['average_pitch'], 1)
            })

        # ë¦¬í¬íŠ¸ ì €ì¥
        voice_analysis_data["final_report"] = report
        voice_analysis_data["analysis_complete"] = True
        
        print("=" * 70)
        print("âœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!")
        print(f"   ğŸ¯ ì¢…í•© ì ìˆ˜: {report['overall_score']}")
        print(f"   ğŸ• íƒ€ì„ìŠ¤íƒ¬í”„: {report['analysis_timestamp']}")
        print(f"   ğŸ¤– AI ë¶„ì„: {report['ai_powered']}")
        print("=" * 70)

        return report
        
    except Exception as e:
        print("=" * 70)
        print(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
        print("=" * 70)
        import traceback
        traceback.print_exc()
        
        # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ë¦¬í¬íŠ¸
        error_report = {
            "overall_score": 0,
            "analysis_timestamp": time.time(),
            "detailed_analysis": {
                "voice_confidence": 0,
                "average_volume": 0,
                "volume_consistency": 0,
                "voice_stability": 0
            },
            "recommendations": [f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"],
            "error": str(e),
            "ai_powered": False
        }
        
        voice_analysis_data["final_report"] = error_report
        voice_analysis_data["analysis_complete"] = True
        
        return error_report


def run_comprehensive_ai_analysis():
    try:
        print("ğŸ¤– AI ì¢…í•© ë¶„ì„ ì‹œì‘...")
        
        audio_file = "final_interview_audio.webm"
        
        if not os.path.exists(audio_file):
            print(f"âš ï¸ ì˜¤ë””ì˜¤ íŒŒì¼ ì—†ìŒ")
            generate_voice_analysis_report()
            return
        
        if LIBROSA_AVAILABLE:
            tremor_analysis = analyze_voice_tremor_librosa(audio_file)
            if tremor_analysis:
                voice_analysis_data["ai_analysis"]["tremor"] = tremor_analysis
        
        if WHISPER_AVAILABLE:
            speech_analysis = analyze_speech_with_whisper_konlpy(audio_file)
            if speech_analysis:
                voice_analysis_data["ai_analysis"]["speech"] = speech_analysis
        
        generate_voice_analysis_report()
        print("âœ… AI ì¢…í•© ë¶„ì„ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ AI ì¢…í•© ë¶„ì„ ì˜¤ë¥˜: {e}")
        generate_voice_analysis_report()

# ë©´ì ‘ ìë™ ì§„í–‰
def auto_next_phase():
    global interview_session
    
    if not interview_session['active']:
        return
    
    current_phase = interview_session['phase']
    
    if current_phase == "thinking":
        interview_session['phase'] = "answering"
        interview_session['phase_start_time'] = time.time()
        
        interview_session['auto_timer'] = threading.Timer(
            interview_session['answer_time'], 
            auto_next_phase
        )
        interview_session['auto_timer'].start()
        
    elif current_phase == "answering":
        next_question_auto()

def next_question_auto():
    global interview_session
    
    if interview_session['question_index'] < len(interview_session['questions_list']):
        current_q = interview_session['questions_list'][interview_session['question_index']]
        
        interview_session.update({
            'question_index': interview_session['question_index'] + 1,
            'current_question': current_q['question'],
            'category': current_q['category'],
            'phase': 'thinking',
            'phase_start_time': time.time(),
            'current_flow_stage': current_q.get('stage', 0)
        })
        
        interview_session['auto_timer'] = threading.Timer(
            interview_session['thinking_time'], 
            auto_next_phase
        )
        interview_session['auto_timer'].start()
        
    else:
        interview_session['phase'] = 'finished'
        voice_analysis_data["session_active"] = False
        threading.Thread(target=run_comprehensive_ai_analysis, daemon=True).start()
        threading.Timer(3.0, stop_interview_internal).start()

def stop_interview_internal():
    global interview_session
    
    if interview_session.get('auto_timer'):
        interview_session['auto_timer'].cancel()
    
    interview_session.update({
        'active': False,
        'current_question': None,
        'phase': 'finished',
        'auto_timer': None,
        'current_flow_stage': 0
    })
    
    voice_analysis_data["session_active"] = False

def generate_structured_interview_questions():
    structured_questions = []
    
    first_question = {
        'category': 'ê¸°ë³¸ì§ˆë¬¸',
        'question': 'ê°„ë‹¨í•˜ê²Œ ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.',
        'stage': 0
    }
    structured_questions.append(first_question)
    
    for stage_idx, category in enumerate(INTERVIEW_FLOW):
        if category == "ê¸°ë³¸ì§ˆë¬¸":
            continue
        
        available_questions = INTERVIEW_QUESTIONS[category]
        selected_question = random.choice(available_questions)
        
        structured_questions.append({
            'category': category,
            'question': selected_question,
            'stage': stage_idx
        })
    
    return structured_questions

# ==================== WebRTC API ì—”ë“œí¬ì¸íŠ¸ë“¤ ====================

@app.route('/health')
def health_check():
    return jsonify({
        'status': 'ok',
        'mode': 'WebRTC',
        'voice_analysis': 'AI-Powered' if (WHISPER_AVAILABLE and LIBROSA_AVAILABLE) else 'Basic',
        'ai_modules': {
            'whisper': WHISPER_AVAILABLE,
            'konlpy': KONLPY_AVAILABLE,
            'librosa': LIBROSA_AVAILABLE,
            'pydub': AUDIO_PROCESSING_AVAILABLE,
            'mediapipe': pose is not None
        }
    })

@app.route('/analyze_frame', methods=['POST'])
def analyze_frame():
    """í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì „ì†¡í•œ í”„ë ˆì„ì„ ë¶„ì„"""
    global latest_analysis_data, is_analysis_active, calibration_in_progress
    
    try:
        data = request.get_json()
        
        if not data or 'frame' not in data:
            return jsonify({'error': 'No frame data'}), 400
        
        # Base64 ë””ì½”ë”©
        frame_data = data['frame'].split(',')[1] if ',' in data['frame'] else data['frame']
        frame_bytes = base64.b64decode(frame_data)
        
        # numpy ë°°ì—´ë¡œ ë³€í™˜
        nparr = np.frombuffer(frame_bytes, np.uint8)
        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if frame is None:
            return jsonify({'error': 'Invalid frame'}), 400
        
        # MediaPipe ìì„¸ ë¶„ì„
        result = {
            'timestamp': time.time(),
            'calibration_in_progress': calibration_in_progress,
            'analysis_active': is_analysis_active
        }
        
        if pose:
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pose_results = pose.process(rgb_frame)
            
            if pose_results.pose_landmarks:
                if calibration_in_progress:
                    analyzer.add_calibration_sample(pose_results.pose_landmarks.landmark)
                    result['calibration_status'] = 'collecting'
                    result['samples_collected'] = len(analyzer.calibration_samples)
                    
                elif analyzer.calibrated and is_analysis_active:
                    analysis = analyzer.analyze_posture(pose_results.pose_landmarks.landmark)
                    
                    # ë©´ì ‘ ì •ë³´
                    current_time = time.time()
                    time_remaining = 0
                    if interview_session['phase_start_time']:
                        elapsed = int(current_time - interview_session['phase_start_time'])
                        if interview_session['phase'] == 'thinking':
                            time_remaining = max(0, interview_session['thinking_time'] - elapsed)
                        elif interview_session['phase'] == 'answering':
                            time_remaining = max(0, interview_session['answer_time'] - elapsed)
                    
                    interview_info = {
                        'active': interview_session["active"],
                        'current_question': interview_session["current_question"],
                        'category': interview_session["category"],
                        'question_number': interview_session["question_index"],
                        'total_questions': len(interview_session["questions_list"]),
                        'phase': interview_session["phase"],
                        'time_remaining': time_remaining,
                        'current_stage': INTERVIEW_FLOW[interview_session.get('current_flow_stage', 0)]
                    }
                    
                    latest_analysis_data = {
                        'score': analysis['score'],
                        'issues': analysis['issues'],
                        'timestamp': time.time(),
                        'interview': interview_info
                    }
                    
                    result.update({
                        'posture_score': analysis['score'],
                        'issues': analysis['issues'],
                        'in_frame': analysis['in_frame'],
                        'interview': interview_info
                    })
            else:
                result['error'] = 'No pose detected'
        
        return jsonify(result)
        
    except Exception as e:
        print(f"âŒ í”„ë ˆì„ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/start_analysis', methods=['POST'])
def start_analysis():
    global is_analysis_active, calibration_in_progress, voice_analysis_data
    
    print("=" * 70)
    print("ğŸ¯ [START_ANALYSIS] í˜¸ì¶œë¨")
    print("=" * 70)
    
    # ìŒì„± ë¶„ì„ ë°ì´í„° ì™„ì „ ì´ˆê¸°í™”
    print("ğŸ§¹ ìŒì„± ë°ì´í„° ì™„ì „ ì´ˆê¸°í™” ì¤‘...")
    voice_analysis_data.clear()
    voice_analysis_data.update({
        "session_active": False,
        "start_time": None,
        "audio_samples": [],
        "filler_words": [],
        "voice_confidence": [],
        "speaking_pace": [],
        "volume_levels": [],
        "total_speaking_time": 0,
        "silence_periods": [],
        "analysis_complete": False,
        "final_report": {},
        "chunk_count": 0,
        "ai_analysis": {}
    })
    
    print(f"âœ… ì´ˆê¸°í™” ì™„ë£Œ:")
    print(f"   - session_active: {voice_analysis_data['session_active']}")
    print(f"   - analysis_complete: {voice_analysis_data['analysis_complete']}")
    print(f"   - final_report: {bool(voice_analysis_data['final_report'])}")
    
    analyzer.reset_calibration()
    calibration_in_progress = True
    is_analysis_active = False
    
    print("=" * 70)
    return jsonify({'status': 'calibration_started'})

@app.route('/finalize_calibration', methods=['POST'])
def finalize_calibration():
    global calibration_in_progress, is_analysis_active
    
    success = analyzer.finalize_calibration()
    
    if success:
        calibration_in_progress = False
        is_analysis_active = True
        return jsonify({'status': 'calibration_complete'})
    else:
        return jsonify({'status': 'calibration_failed', 'message': 'ìƒ˜í”Œ ë¶€ì¡±'}), 400

@app.route('/stop_analysis', methods=['POST'])
def stop_analysis():
    global is_analysis_active, calibration_in_progress, voice_analysis_data
    
    print("=" * 70)
    print("ğŸ›‘ ë¶„ì„ ì¤‘ì§€ ìš”ì²­")
    print("=" * 70)
    
    is_analysis_active = False
    calibration_in_progress = False
    
    # ë©´ì ‘ ì¤‘ì§€
    if interview_session.get('auto_timer'):
        try:
            interview_session['auto_timer'].cancel()
            print("âœ… ë©´ì ‘ íƒ€ì´ë¨¸ ì·¨ì†Œ")
        except:
            pass
    
    interview_session.update({
        'active': False, 
        'current_question': None,
        'phase': 'finished', 
        'auto_timer': None
    })
    
    # ìŒì„± ë¶„ì„ ë°ì´í„° ì™„ì „ ì´ˆê¸°í™”
    print("ğŸ§¹ ìŒì„± ë¶„ì„ ë°ì´í„° ì´ˆê¸°í™”")
    voice_analysis_data.clear()
    voice_analysis_data.update({
        "session_active": False,
        "start_time": None,
        "audio_samples": [],
        "filler_words": [],
        "voice_confidence": [],
        "speaking_pace": [],
        "volume_levels": [],
        "total_speaking_time": 0,
        "silence_periods": [],
        "analysis_complete": False,
        "final_report": {},
        "chunk_count": 0,
        "ai_analysis": {}
    })
    
    print("=" * 70)
    print("âœ… ë¶„ì„ ì¤‘ì§€ ì™„ë£Œ")
    print("=" * 70)
    
    return jsonify({
        'status': 'stopped',
        'voice_data_reset': True
    })

@app.route('/interview/start', methods=['POST'])
def start_interview():
    global interview_session, voice_analysis_data
    
    print("=" * 70)
    print("ğŸ¬ [INTERVIEW/START] í˜¸ì¶œë¨")
    print("=" * 70)
    
    # 1. ì´ì „ íƒ€ì´ë¨¸ ì·¨ì†Œ
    if interview_session.get('auto_timer'):
        try:
            interview_session['auto_timer'].cancel()
            print("âœ… ì´ì „ íƒ€ì´ë¨¸ ì·¨ì†Œ")
        except:
            pass
    
    # 2. ì´ì „ ì˜¤ë””ì˜¤ íŒŒì¼ ì‚­ì œ
    print("ğŸ§¹ ì˜¤ë””ì˜¤ íŒŒì¼ ì‚­ì œ ì¤‘...")
    try:
        if os.path.exists("final_interview_audio.webm"):
            os.remove("final_interview_audio.webm")
            print("   âœ… final_interview_audio.webm ì‚­ì œ")
        
        deleted_chunks = 0
        for f in os.listdir('.'):
            if f.startswith('temp_chunk_') and f.endswith('.webm'):
                try:
                    os.remove(f)
                    deleted_chunks += 1
                except:
                    pass
        
        if deleted_chunks > 0:
            print(f"   âœ… {deleted_chunks}ê°œ ì„ì‹œ ì²­í¬ ì‚­ì œ")
            
    except Exception as e:
        print(f"   âš ï¸ íŒŒì¼ ì‚­ì œ ì˜¤ë¥˜: {e}")
    
    # 3. voice_analysis_data ì™„ì „ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ!)
    print("ğŸ”„ ìŒì„± ë°ì´í„° ì™„ì „ ì´ˆê¸°í™” ì¤‘...")
    voice_analysis_data.clear()
    voice_analysis_data.update({
        "session_active": True,
        "start_time": time.time(),
        "audio_samples": [],
        "filler_words": [],
        "voice_confidence": [],
        "speaking_pace": [],
        "volume_levels": [],
        "total_speaking_time": 0,
        "silence_periods": [],
        "analysis_complete": False,
        "final_report": {},
        "chunk_count": 0,
        "ai_analysis": {}
    })
    
    print("âœ… ì´ˆê¸°í™” ì™„ë£Œ:")
    print(f"   - session_active: {voice_analysis_data['session_active']}")
    print(f"   - analysis_complete: {voice_analysis_data['analysis_complete']}")
    print(f"   - final_report: {bool(voice_analysis_data['final_report'])}")
    
    # 4. ë©´ì ‘ ì§ˆë¬¸ ìƒì„±
    print("ğŸ“ ì§ˆë¬¸ ìƒì„± ì¤‘...")
    structured_questions = generate_structured_interview_questions()
    first_q = structured_questions[0]
    print(f"âœ… {len(structured_questions)}ê°œ ì§ˆë¬¸ ìƒì„±")
    
    # 5. ë©´ì ‘ ì„¸ì…˜ ì„¤ì •
    interview_session.update({
        'active': True,
        'questions_list': structured_questions,
        'question_index': 1,
        'current_question': first_q['question'],
        'category': first_q['category'],
        'phase': 'thinking',
        'phase_start_time': time.time(),
        'current_flow_stage': first_q['stage']
    })
    
    # 6. ìë™ ì§„í–‰ íƒ€ì´ë¨¸ ì‹œì‘
    interview_session['auto_timer'] = threading.Timer(
        interview_session['thinking_time'], 
        auto_next_phase
    )
    interview_session['auto_timer'].start()
    
    print("=" * 70)
    print(f"ğŸ‰ ë©´ì ‘ ì‹œì‘ ì™„ë£Œ - ì´ {len(structured_questions)}ê°œ ì§ˆë¬¸")
    print("=" * 70)
    
    return jsonify({
        'status': 'started',
        'total_questions': len(structured_questions),
        'current_question': first_q['question'],
        'category': first_q['category'],
        'flow_stage': INTERVIEW_FLOW[first_q['stage']],
        'voice_analysis_active': True,
        'ai_enabled': WHISPER_AVAILABLE and LIBROSA_AVAILABLE,
        'voice_data_reset': True
    })

@app.route('/interview/stop', methods=['POST'])
def stop_interview():
    stop_interview_internal()
    return jsonify({'status': 'stopped'})

@app.route('/interview/status', methods=['GET'])
def interview_status():
    current_time = time.time()
    time_remaining = 0
    
    if interview_session['phase_start_time']:
        elapsed = int(current_time - interview_session['phase_start_time'])
        if interview_session['phase'] == 'thinking':
            time_remaining = max(0, interview_session['thinking_time'] - elapsed)
        elif interview_session['phase'] == 'answering':
            time_remaining = max(0, interview_session['answer_time'] - elapsed)
    
    return jsonify({
        'active': interview_session['active'],
        'current_question': interview_session['current_question'],
        'category': interview_session['category'],
        'question_number': interview_session['question_index'],
        'total_questions': len(interview_session['questions_list']),
        'phase': interview_session['phase'],
        'time_remaining': time_remaining,
        'current_stage': INTERVIEW_FLOW[interview_session.get('current_flow_stage', 0)] if interview_session['active'] else None
    })

# ìŒì„± ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸ (ê¸°ì¡´ê³¼ ë™ì¼)
@app.route('/voice/audio_chunk_blob', methods=['POST'])
def receive_audio_chunk_blob():
    try:
        if not voice_analysis_data["session_active"]:
            return jsonify({'status': 'session_inactive'})
        
        if 'audio' not in request.files:
            return jsonify({'error': 'No audio file'})
        
        audio_file = request.files['audio']
        chunk_number = int(request.form.get('chunk_number', 0))
        audio_data = audio_file.read()
        voice_analysis_data["chunk_count"] += 1
        
        chunk_path = f"temp_chunk_{chunk_number}.webm"
        with open(chunk_path, 'wb') as f:
            f.write(audio_data)
        
        if len(audio_data) > 100:
            basic_volume = min(100, (len(audio_data) / 1000) * 8)
            basic_sample = {
                'volume': basic_volume / 100,
                'is_speaking': True,
                'timestamp': time.time()
            }
            voice_analysis_data["audio_samples"].append(basic_sample)
        
        return jsonify({
            'status': 'success',
            'chunk_number': chunk_number,
            'samples_total': len(voice_analysis_data["audio_samples"])
        })
        
    except Exception as e:
        print(f"âŒ ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return jsonify({'error': str(e)})

@app.route('/voice/final_audio', methods=['POST'])
def receive_final_audio():
    try:
        if 'final_audio' not in request.files:
            return jsonify({'error': 'No final audio file'})
        
        audio_file = request.files['final_audio']
        final_audio_path = "final_interview_audio.webm"
        audio_file.save(final_audio_path)
        
        file_size = os.path.getsize(final_audio_path)
        
        if WHISPER_AVAILABLE or LIBROSA_AVAILABLE:
            threading.Thread(target=run_comprehensive_ai_analysis, daemon=True).start()
        
        return jsonify({
            'status': 'final_audio_received',
            'file_size': file_size,
            'ai_analysis_started': WHISPER_AVAILABLE or LIBROSA_AVAILABLE
        })
        
    except Exception as e:
        print(f"âŒ ìµœì¢… ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return jsonify({'error': str(e)})

@app.route('/voice/analysis_report', methods=['GET'])
def get_voice_analysis_report():
    """ìŒì„± ë¶„ì„ ë¦¬í¬íŠ¸ ì¡°íšŒ"""
    
    print("=" * 50)
    print("ğŸ“¥ [ANALYSIS_REPORT] ìš”ì²­ ìˆ˜ì‹ ")
    print(f"   - session_active: {voice_analysis_data.get('session_active')}")
    print(f"   - analysis_complete: {voice_analysis_data.get('analysis_complete')}")
    print(f"   - final_report ì¡´ì¬: {bool(voice_analysis_data.get('final_report'))}")
    
    # ===== í•µì‹¬: ì„¸ì…˜ í™œì„± ì¤‘ì—ëŠ” ë¦¬í¬íŠ¸ ë°˜í™˜ ì•ˆ í•¨ =====
    if voice_analysis_data.get("session_active"):
        print("   âš ï¸ ì„¸ì…˜ í™œì„± ì¤‘ - ë¦¬í¬íŠ¸ ì—†ìŒ")
        print("=" * 50)
        return jsonify({
            'status': 'session_active',
            'message': 'ë©´ì ‘ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.',
            'analysis_complete': False
        })
    
    # analysis_complete ì²´í¬
    if voice_analysis_data.get("analysis_complete") and voice_analysis_data.get("final_report"):
        report = voice_analysis_data["final_report"]
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ í™•ì¸
        if 'analysis_timestamp' not in report:
            report['analysis_timestamp'] = time.time()
            print("   âš ï¸ íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ìŒ - ìƒì„±")
        
        print(f"   âœ… ë¦¬í¬íŠ¸ ì „ì†¡:")
        print(f"      - ì ìˆ˜: {report.get('overall_score')}")
        print(f"      - íƒ€ì„ìŠ¤íƒ¬í”„: {report.get('analysis_timestamp')}")
        print(f"      - AI ë¶„ì„: {report.get('ai_powered')}")
        print("=" * 50)
        
        return jsonify(report)
    else:
        # ì•„ì§ ë¶„ì„ ì¤‘
        samples = len(voice_analysis_data.get("audio_samples", []))
        chunks = voice_analysis_data.get("chunk_count", 0)
        
        print(f"   â³ ë¦¬í¬íŠ¸ ì¤€ë¹„ ì¤‘:")
        print(f"      - ìƒ˜í”Œ: {samples}ê°œ")
        print(f"      - ì²­í¬: {chunks}ê°œ")
        print("=" * 50)
        
        return jsonify({
            'status': 'analysis_in_progress',
            'message': 'AI ìŒì„± ë¶„ì„ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.',
            'ai_enabled': WHISPER_AVAILABLE and LIBROSA_AVAILABLE,
            'samples': samples,
            'chunks': chunks,
            'analysis_complete': False
        })

# ë¦¬ì†ŒìŠ¤ ì •ë¦¬
def cleanup_resources():
    print("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...")
    
    global pose
    if pose:
        try:
            pose.close()
            print("âœ… MediaPipe ë¦¬ì†ŒìŠ¤ í•´ì œ")
        except:
            pass
        pose = None
    
    if interview_session.get('auto_timer'):
        try:
            interview_session['auto_timer'].cancel()
        except:
            pass
    
    try:
        for f in os.listdir('.'):
            if f.startswith('temp_chunk_') or f == 'final_interview_audio.webm':
                os.remove(f)
    except:
        pass
    
    print("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

atexit.register(cleanup_resources)

if __name__ == '__main__':
    print("=" * 90)
    print("ğŸ“ AI ê¸°ë°˜ ì²´ìœ¡ëŒ€í•™ ë©´ì ‘ ë¶„ì„ ì‹œìŠ¤í…œ (WebRTC ëª¨ë“œ)")
    print("=" * 90)
    print(f"ğŸŒ ì„œë²„ ì£¼ì†Œ: http://0.0.0.0:5001")
    print("ğŸ“± ëª¨ë“œ: WebRTC (í´ë¼ì´ì–¸íŠ¸ ë¸Œë¼ìš°ì € ì¹´ë©”ë¼ ì‚¬ìš©)")
    print("\nğŸ¤– AI ëª¨ë“ˆ ìƒíƒœ:")
    print(f"   - Whisper: {'âœ…' if WHISPER_AVAILABLE else 'âŒ'}")
    print(f"   - KoNLPy: {'âœ…' if KONLPY_AVAILABLE else 'âŒ'}")
    print(f"   - librosa: {'âœ…' if LIBROSA_AVAILABLE else 'âŒ'}")
    print(f"   - MediaPipe: {'âœ…' if pose else 'âŒ'}")
    print("=" * 90)

    try:
        app.run(host='0.0.0.0', port=5001, debug=False, threaded=True)
    except KeyboardInterrupt:
        print("\nğŸ›‘ ì„œë²„ ì¢…ë£Œ ì¤‘...")
        cleanup_resources()
    except Exception as e:
        print(f"\nğŸ’¥ ì˜¤ë¥˜: {e}")
        cleanup_resources()