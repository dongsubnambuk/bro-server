# server.py - Whisper + KoNLPy + librosa í†µí•© ìŒì„± ë¶„ì„ ì‹œìŠ¤í…œ
# ì„¤ì¹˜: pip install mediapipe opencv-python flask flask-cors numpy pydub openai-whisper konlpy librosa soundfile

from flask import Flask, Response, jsonify, request
from flask_cors import CORS
import cv2
import mediapipe as mp
import numpy as np
import json
import time
from collections import deque, Counter
import threading
import random
import atexit
import io
import os

# AI ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from pydub import AudioSegment
    AUDIO_PROCESSING_AVAILABLE = True
    print("âœ… pydub ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")
except ImportError:
    AUDIO_PROCESSING_AVAILABLE = False
    print("âš ï¸ pydub ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

try:
    import whisper
    WHISPER_AVAILABLE = True
    print("âœ… Whisper ë¡œë“œ ì™„ë£Œ")
except ImportError:
    WHISPER_AVAILABLE = False
    print("âš ï¸ Whisperê°€ ì—†ìŠµë‹ˆë‹¤. pip install openai-whisper")

try:
    from konlpy.tag import Okt
    KONLPY_AVAILABLE = False
    print("âœ… KoNLPy ë¡œë“œ ì™„ë£Œ")
except ImportError:
    KONLPY_AVAILABLE = False
    print("âš ï¸ KoNLPyê°€ ì—†ìŠµë‹ˆë‹¤. pip install konlpy")

try:
    import librosa
    import soundfile as sf
    LIBROSA_AVAILABLE = True
    print("âœ… librosa ë¡œë“œ ì™„ë£Œ")
except ImportError:
    LIBROSA_AVAILABLE = False
    print("âš ï¸ librosaê°€ ì—†ìŠµë‹ˆë‹¤. pip install librosa soundfile")

app = Flask(__name__)
CORS(app)

mp_pose = mp.solutions.pose

# MediaPipe ì´ˆê¸°í™”
try:
    pose = mp_pose.Pose(
        static_image_mode=False,
        model_complexity=1,
        smooth_landmarks=True,
        enable_segmentation=False,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    )
    print("âœ… MediaPipe Pose ì´ˆê¸°í™” ì™„ë£Œ")
except Exception as e:
    print(f"âŒ MediaPipe Pose ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    pose = None

# AI ëª¨ë¸ ì´ˆê¸°í™”
whisper_model = None
okt = None

if WHISPER_AVAILABLE:
    print("ğŸ”„ Whisper ëª¨ë¸ ë¡œë”© ì¤‘... (ìµœì´ˆ 1íšŒ, ì•½ 30ì´ˆ ì†Œìš”)")
    whisper_model = whisper.load_model("tiny")  # tiny/base/small ì¤‘ ì„ íƒ
    print("âœ… Whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

if KONLPY_AVAILABLE:
    okt = Okt()
    print("âœ… KoNLPy Okt ì´ˆê¸°í™” ì™„ë£Œ")

# ìŒì„± ë¶„ì„ ê²°ê³¼ ì €ì¥
voice_analysis_data = {
    "session_active": False,
    "start_time": None,
    "audio_samples": [],
    "filler_words": [],
    "voice_confidence": [],
    "speaking_pace": [],
    "volume_levels": [],
    "total_speaking_time": 0,
    "silence_periods": [],
    "analysis_complete": False,
    "final_report": {},
    "chunk_count": 0,
    "ai_analysis": {}  # AI ë¶„ì„ ê²°ê³¼
}

# ì¶”ì„ìƒˆ/í•„ëŸ¬ ë‹¨ì–´ ëª©ë¡ (í™•ì¥)
FILLER_WORDS = [
    "ì–´", "ìŒ", "ì•„", "ê·¸", "ë­", "ì´ì œ", "ê·¸ë˜ì„œ", "ê·¸ë‹ˆê¹Œ", "ê·¸ëŸ°ë°",
    "ì–´ë–»ê²Œ", "ë­”ê°€", "ì•½ê°„", "ì¢€", "ê·¸ê±°", "ì´ê±°", "ì €ê¸°", "ì•„ë¬´íŠ¼",
    "ì¼ë‹¨", "ìš°ì„ ", "ê·¸ëŸ¬ë©´", "ê·¸ëŸ¼", "ì•„ë‹ˆ", "ë§ì•„", "ê·¸ì¹˜", "ì‘", "ë„¤",
    "ì—", "ì—„", "í ", "ì•„ë¬´ë˜ë„"
]

# ì ‘ì†ì‚¬ ëª©ë¡
CONNECTIVES = [
    "ê·¸ë˜ì„œ", "ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ê·¸ë¦¬ê³ ", "ë˜í•œ", "ë”°ë¼ì„œ", "ê·¸ëŸ°ë°",
    "ì™œëƒí•˜ë©´", "ì¦‰", "ê²°êµ­", "ê·¸ëŸ¬ë¯€ë¡œ", "ê·¸ë ‡ì§€ë§Œ", "ë˜ëŠ”", "í˜¹ì€",
    "ê·¸ëŸ¼ì—ë„", "ë°˜ë©´ì—", "í•œí¸", "ë”ë¶ˆì–´"
]

# ì²´ìœ¡ëŒ€í•™ ìœ¡ìƒ ì „ê³µ ë©´ì ‘ ì§ˆë¬¸ ë°ì´í„°ë² ì´ìŠ¤
INTERVIEW_QUESTIONS = {
    "ê¸°ë³¸ì§ˆë¬¸": [
        "ê°„ë‹¨í•˜ê²Œ ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.",
        "ì²´ìœ¡í•™ê³¼ì— ì§€ì›í•œ ë™ê¸°ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ë³¸ì¸ì˜ ì¥ì ê³¼ ë‹¨ì ì„ ì´ì•¼ê¸°í•´ ì£¼ì„¸ìš”.",
        "ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ìš´ë™ ì¢…ëª©ì€ ë¬´ì—‡ì´ê³ , ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ëŒ€í•™êµ 4ë…„ ë™ì•ˆ ë¬´ì—‡ì„ ë°°ìš°ê³  ì‹¶ë‚˜ìš”?"
    ],
    "ì§€ì›ë™ê¸°": [
        "ìš°ë¦¬ í•™êµ ì²´ìœ¡í•™ê³¼ë¥¼ ì„ íƒí•œ íŠ¹ë³„í•œ ì´ìœ ê°€ ìˆë‚˜ìš”?",
        "ì²´ìœ¡êµì‚¬ë‚˜ ìŠ¤í¬ì¸ ì§€ë„ìê°€ ë˜ê³  ì‹¶ì€ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ì¡¸ì—… í›„ ì§„ë¡œ ê³„íšì— ëŒ€í•´ êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ì£¼ì„¸ìš”.",
        "ì²´ìœ¡ ë¶„ì•¼ì—ì„œ ë³¸ì¸ë§Œì˜ ëª©í‘œë‚˜ ë¹„ì „ì´ ìˆë‹¤ë©´ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì´ ë¶„ì•¼ë¥¼ ì„ íƒí•˜ê²Œ ëœ ê³„ê¸°ë‚˜ ì˜í–¥ì„ ë°›ì€ ì‚¬ëŒì´ ìˆë‚˜ìš”?"
    ],
    "ìœ¡ìƒì „ë¬¸": [
        "ìœ¡ìƒ ìš´ë™ì„ ì‹œì‘í•˜ê²Œ ëœ ê³„ê¸°ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ë³¸ì¸ì´ ê°€ì¥ ìì‹  ìˆëŠ” ìœ¡ìƒ ì¢…ëª©ì€ ë¬´ì—‡ì´ê³ , ì–´ë–¤ ê¸°ë¡ì„ ê°€ì§€ê³  ìˆë‚˜ìš”?",
        "ìœ¡ìƒ ìš´ë™ì˜ ë§¤ë ¥ì€ ë¬´ì—‡ì´ë¼ê³  ìƒê°í•˜ë‚˜ìš”?",
        "ë‹¨ê±°ë¦¬ì™€ ì¥ê±°ë¦¬ ìœ¡ìƒì˜ ì°¨ì´ì ì— ëŒ€í•´ ì„¤ëª…í•´ë³´ì„¸ìš”.",
        "ìœ¡ìƒ ì„ ìˆ˜ì—ê²Œ í•„ìš”í•œ ì²´ë ¥ ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ì¢‹ì•„í•˜ëŠ” ìœ¡ìƒ ì„ ìˆ˜ê°€ ìˆë‹¤ë©´ ëˆ„êµ¬ì´ê³  ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
    ],
    "ìƒí™©ì§ˆë¬¸": [
        "íŒ€ ë‚´ì—ì„œ ê°ˆë“±ì´ ìƒê²¼ì„ ë•Œ ì–´ë–»ê²Œ í•´ê²°í•˜ì‹œê² ë‚˜ìš”?",
        "ì¤‘ìš”í•œ ëŒ€íšŒë¥¼ ì•ë‘ê³  ë¶€ìƒì„ ë‹¹í–ˆë‹¤ë©´ ì–´ë–»ê²Œ í•˜ì‹œê² ë‚˜ìš”?",
        "í›ˆë ¨ì´ í˜ë“¤ì–´ì„œ í¬ê¸°í•˜ê³  ì‹¶ì„ ë•ŒëŠ” ì–´ë–»ê²Œ ê·¹ë³µí•˜ë‚˜ìš”?",
        "ëª©í‘œë¥¼ ë‹¬ì„±í•˜ì§€ ëª»í–ˆì„ ë•Œì˜ ê²½í—˜ê³¼ ê·¹ë³µ ë°©ë²•ì„ ë§í•´ë³´ì„¸ìš”.",
        "í›„ë°°ë‚˜ ë™ë£Œì™€ ì˜ê²¬ ì°¨ì´ê°€ ìˆì„ ë•Œ ì–´ë–»ê²Œ í•´ê²°í•˜ë‚˜ìš”?",
        "ì‹¤ìˆ˜ë¥¼ í–ˆì„ ë•Œ ì–´ë–»ê²Œ ëŒ€ì²˜í•˜ë‚˜ìš”?"
    ],
    "ì••ë°•ì§ˆë¬¸": [
        "ë‹¤ë¥¸ ì§€ì›ìë“¤ê³¼ ë¹„êµí–ˆì„ ë•Œ ë³¸ì¸ì˜ ê²½ìŸë ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì²´ìœ¡ ì„±ì ì´ ì¢‹ì§€ ì•Šë‹¤ë©´ ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ìš´ë™ì„ ì‹«ì–´í•˜ëŠ” í•™ìƒì—ê²Œ ì–´ë–»ê²Œ ë™ê¸°ë¶€ì—¬ë¥¼ í•  ê²ƒì¸ê°€ìš”?",
        "ì²´ìœ¡ ìˆ˜ì—… ì‹œê°„ì— ë‹¤ì¹œ í•™ìƒì´ ìˆë‹¤ë©´ ì–´ë–»ê²Œ ëŒ€ì²˜í•˜ì‹œê² ë‚˜ìš”?",
        "ë³¸ì¸ì˜ ê°€ì¥ í° ì•½ì ì€ ë¬´ì—‡ì´ê³ , ì–´ë–»ê²Œ ê°œì„ í•˜ê³  ìˆë‚˜ìš”?",
        "ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ì„ ë•Œ ì–´ë–»ê²Œ í•´ì†Œí•˜ì‹œë‚˜ìš”?"
    ],
    "ë§ˆë¬´ë¦¬ì§ˆë¬¸": [
        "ìš°ë¦¬ í•™êµì— ì…í•™í•˜ë©´ ê°€ì¥ ë¨¼ì € í•˜ê³  ì‹¶ì€ ì¼ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "10ë…„ í›„ ë³¸ì¸ì˜ ëª¨ìŠµì„ ì–´ë–»ê²Œ ê·¸ë ¤ë³´ê³  ìˆë‚˜ìš”?",
        "ì²´ìœ¡ ì§€ë„ìë¡œì„œ ê°€ì¥ ë³´ëŒ ìˆì„ ê²ƒ ê°™ì€ ìˆœê°„ì€ ì–¸ì œì¸ê°€ìš”?",
        "ë§ˆì§€ë§‰ìœ¼ë¡œ í•˜ê³  ì‹¶ì€ ë§ì”€ì´ ìˆë‹¤ë©´ í•´ì£¼ì„¸ìš”.",
        "ìš°ë¦¬ì—ê²Œ ê¼­ ë¬¼ì–´ë³´ê³  ì‹¶ì€ ê²ƒì´ ìˆë‚˜ìš”?"
    ]
}

INTERVIEW_FLOW = [
    "ê¸°ë³¸ì§ˆë¬¸", "ì§€ì›ë™ê¸°", "ìœ¡ìƒì „ë¬¸", "ìƒí™©ì§ˆë¬¸", "ì••ë°•ì§ˆë¬¸", "ë§ˆë¬´ë¦¬ì§ˆë¬¸"
]

# ë©´ì ‘ ì„¸ì…˜ ê´€ë¦¬
interview_session = {
    "active": False,
    "current_question": None,
    "question_index": 0,
    "category": None,
    "questions_list": [],
    "start_time": None,
    "thinking_time": 10,
    "answer_time": 5,
    "phase": "thinking",
    "auto_timer": None,
    "phase_start_time": None,
    "current_flow_stage": 0
}

# ==================== AI ìŒì„± ë¶„ì„ í•¨ìˆ˜ë“¤ ====================

def analyze_voice_tremor_librosa(audio_path):
    """librosaë¡œ ìŒì • ë–¨ë¦¼ ë¶„ì„"""
    if not LIBROSA_AVAILABLE:
        print("âš ï¸ librosaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê¸°ë³¸ ë¶„ì„ ì‚¬ìš©")
        return None
    
    try:
        print(f"ğŸ”¬ librosa ìŒì • ë–¨ë¦¼ ë¶„ì„ ì‹œì‘: {audio_path}")
        y, sr = librosa.load(audio_path, sr=None)
        
        if len(y) < sr * 0.5:  # 0.5ì´ˆ ë¯¸ë§Œì´ë©´ ìŠ¤í‚µ
            print("âš ï¸ ì˜¤ë””ì˜¤ê°€ ë„ˆë¬´ ì§§ìŒ")
            return None
        
        # í”¼ì¹˜ ì¶”ì¶œ
        pitches, magnitudes = librosa.piptrack(y=y, sr=sr, fmin=75, fmax=300)
        pitch_values = []
        
        for t in range(pitches.shape[1]):
            index = magnitudes[:, t].argmax()
            pitch = pitches[index, t]
            if pitch > 0:
                pitch_values.append(pitch)
        
        if len(pitch_values) > 1:
            pitch_changes = np.diff(pitch_values)
            jitter = np.std(pitch_changes)
            jitter_percent = (jitter / np.mean(pitch_values)) * 100
        else:
            jitter = 0
            jitter_percent = 0
        
        # ìŒëŸ‰ ë–¨ë¦¼ (Shimmer)
        rms = librosa.feature.rms(y=y)[0]
        shimmer = np.std(rms)
        
        # ìŠ¤í™íŠ¸ëŸ´ ì•ˆì •ì„±
        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
        spectral_stability = np.std(spectral_centroid)
        
        # ìì‹ ê° ì ìˆ˜ ê³„ì‚°
        if jitter_percent < 1.0 and shimmer < 0.02:
            confidence = 95
        elif jitter_percent < 2.0 and shimmer < 0.04:
            confidence = 80
        elif jitter_percent < 3.5 and shimmer < 0.06:
            confidence = 60
        else:
            confidence = 40
        
        result = {
            'jitter': float(jitter),
            'jitter_percent': float(jitter_percent),
            'shimmer': float(shimmer),
            'spectral_stability': float(spectral_stability),
            'average_pitch': float(np.mean(pitch_values)) if pitch_values else 0,
            'pitch_range': float(np.max(pitch_values) - np.min(pitch_values)) if len(pitch_values) > 1 else 0,
            'voice_confidence': confidence
        }
        
        print(f"âœ… librosa ë¶„ì„ ì™„ë£Œ: Jitter={jitter_percent:.2f}%, ìì‹ ê°={confidence}")
        return result
        
    except Exception as e:
        print(f"âŒ librosa ë¶„ì„ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None

def analyze_speech_with_whisper_konlpy(audio_path):
    """Whisper + KoNLPy ì¢…í•© í…ìŠ¤íŠ¸ ë¶„ì„"""
    if not WHISPER_AVAILABLE:
        print("âš ï¸ Whisperê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ í…ìŠ¤íŠ¸ ë¶„ì„ ë¶ˆê°€")
        return None
    
    try:
        print(f"ğŸ¤ Whisper ìŒì„± ì¸ì‹ ì‹œì‘: {audio_path}")
        
        # Whisperë¡œ ìŒì„± ì¸ì‹
        result = whisper_model.transcribe(audio_path, language='ko')
        full_text = result['text']
        
        print(f"ğŸ“ ì¸ì‹ëœ í…ìŠ¤íŠ¸ ({len(full_text)}ì): {full_text[:100]}...")
        
        if not full_text.strip():
            print("âš ï¸ ì¸ì‹ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŒ")
            return None
        
        # ê¸°ë³¸ ë‹¨ì–´ ë¶„ì„
        words = full_text.split()
        
        # ì¶”ì„ìƒˆ ë¶„ì„
        filler_count = Counter()
        for word in words:
            clean_word = word.strip('.,!?').lower()
            if clean_word in FILLER_WORDS:
                filler_count[clean_word] += 1
        
        # ì ‘ì†ì‚¬ ë¶„ì„
        connective_count = Counter()
        for word in words:
            clean_word = word.strip('.,!?').lower()
            if clean_word in CONNECTIVES:
                connective_count[clean_word] += 1
        
        # KoNLPy í˜•íƒœì†Œ ë¶„ì„
        nouns = []
        verbs = []
        adjectives = []
        adverbs = []
        
        if KONLPY_AVAILABLE and okt:
            print("ğŸ” KoNLPy í’ˆì‚¬ ë¶„ì„ ì‹œì‘...")
            try:
                morphs = okt.pos(full_text)
                nouns = [word for word, pos in morphs if pos == 'Noun']
                verbs = [word for word, pos in morphs if pos == 'Verb']
                adjectives = [word for word, pos in morphs if pos == 'Adjective']
                adverbs = [word for word, pos in morphs if pos == 'Adverb']
                print(f"âœ… KoNLPy ë¶„ì„ ì™„ë£Œ: ëª…ì‚¬={len(nouns)}, ë™ì‚¬={len(verbs)}")
            except Exception as e:
                print(f"âš ï¸ KoNLPy ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        # ë°˜ë³µ ë‹¨ì–´ ë¶„ì„
        word_freq = Counter(word.strip('.,!?').lower() for word in words if len(word) > 1)
        repeated_words = {word: count for word, count in word_freq.items() if count > 2}
        
        # ë¬¸ì¥ ë¶„ì„
        sentences = [s.strip() for s in full_text.split('.') if s.strip()]
        avg_sentence_length = len(words) / len(sentences) if sentences else 0
        
        # ì–´íœ˜ ë‹¤ì–‘ì„±
        vocabulary_richness = len(set(nouns)) / len(nouns) if nouns else 0
        
        analysis_result = {
            'full_text': full_text,
            'total_words': len(words),
            
            # ì¶”ì„ìƒˆ
            'filler_words': dict(filler_count),
            'filler_total': sum(filler_count.values()),
            'filler_ratio': (sum(filler_count.values()) / len(words) * 100) if words else 0,
            
            # ì ‘ì†ì‚¬
            'connectives': dict(connective_count),
            'connective_total': sum(connective_count.values()),
            
            # í’ˆì‚¬ë³„
            'noun_count': len(nouns),
            'verb_count': len(verbs),
            'adjective_count': len(adjectives),
            'adverb_count': len(adverbs),
            'top_nouns': dict(Counter(nouns).most_common(10)),
            
            # ë°˜ë³µ ë‹¨ì–´
            'repeated_words': repeated_words,
            
            # ë¬¸ì¥
            'sentence_count': len(sentences),
            'avg_sentence_length': avg_sentence_length,
            
            # ì–´íœ˜ë ¥
            'vocabulary_richness': vocabulary_richness * 100,
            
            # ì‹œê°„ë³„ ì„¸ê·¸ë¨¼íŠ¸
            'segments': result.get('segments', [])
        }
        
        print(f"âœ… Whisper+KoNLPy ë¶„ì„ ì™„ë£Œ")
        print(f"   - ì¶”ì„ìƒˆ: {analysis_result['filler_total']}íšŒ ({analysis_result['filler_ratio']:.1f}%)")
        print(f"   - ì ‘ì†ì‚¬: {analysis_result['connective_total']}íšŒ")
        print(f"   - ì–´íœ˜ ë‹¤ì–‘ì„±: {analysis_result['vocabulary_richness']:.1f}%")
        
        return analysis_result
        
    except Exception as e:
        print(f"âŒ Whisper+KoNLPy ë¶„ì„ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None

def generate_ai_recommendations(tremor, speech):
    """AI ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ìƒì„±"""
    recommendations = []
    
    if tremor:
        # ìŒì„± ë–¨ë¦¼
        if tremor['jitter_percent'] > 3.0:
            recommendations.append(f"ëª©ì†Œë¦¬ì— ë–¨ë¦¼ì´ ìˆìŠµë‹ˆë‹¤ (Jitter: {tremor['jitter_percent']:.1f}%). ì‹¬í˜¸í¡ì„ í•˜ê³  ì²œì²œíˆ ë§í•´ë³´ì„¸ìš”.")
        elif tremor['jitter_percent'] > 2.0:
            recommendations.append("ì•½ê°„ì˜ ê¸´ì¥ì´ ëŠê»´ì§‘ë‹ˆë‹¤. í¸ì•ˆí•˜ê²Œ ë‹µë³€í•´ë³´ì„¸ìš”.")
        
        if tremor['shimmer'] > 0.05:
            recommendations.append("ìŒëŸ‰ ë³€í™”ê°€ í½ë‹ˆë‹¤. ì¼ì •í•œ í¬ê¸°ë¡œ ë§í•´ë³´ì„¸ìš”.")
    
    if speech:
        # ì¶”ì„ìƒˆ
        if speech['filler_ratio'] > 10:
            recommendations.append(f"ì¶”ì„ìƒˆê°€ ë§ìŠµë‹ˆë‹¤ ({speech['filler_total']}íšŒ, {speech['filler_ratio']:.1f}%). ì˜ì‹ì ìœ¼ë¡œ ì¤„ì—¬ë³´ì„¸ìš”.")
            if speech['filler_words']:
                top_fillers = sorted(speech['filler_words'].items(), key=lambda x: x[1], reverse=True)[:3]
                recommendations.append(f"ê°€ì¥ ë§ì´ ì‚¬ìš©í•œ ì¶”ì„ìƒˆ: {', '.join([f'{w}({c}íšŒ)' for w, c in top_fillers])}")
        elif speech['filler_ratio'] > 5:
            recommendations.append("ì¶”ì„ìƒˆë¥¼ ì¡°ê¸ˆ ì¤„ì´ë©´ ë” ì¢‹ìŠµë‹ˆë‹¤.")
        
        # ì ‘ì†ì‚¬
        if speech['connective_total'] > 15:
            recommendations.append(f"ì ‘ì†ì‚¬ ì‚¬ìš©ì´ ë§ìŠµë‹ˆë‹¤ ({speech['connective_total']}íšŒ). ë¬¸ì¥ì„ ê°„ê²°í•˜ê²Œ ë§Œë“¤ì–´ë³´ì„¸ìš”.")
        
        # ë°˜ë³µ ë‹¨ì–´
        if speech.get('repeated_words'):
            top_repeated = sorted(speech['repeated_words'].items(), key=lambda x: x[1], reverse=True)[:3]
            if top_repeated:
                recommendations.append(f"ë°˜ë³µëœ ë‹¨ì–´: {', '.join([f'{w}({c}íšŒ)' for w, c in top_repeated])}")
        
        # ì–´íœ˜ ë‹¤ì–‘ì„±
        if speech['vocabulary_richness'] < 30:
            recommendations.append("ë‹¤ì–‘í•œ ì–´íœ˜ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
        elif speech['vocabulary_richness'] > 70:
            recommendations.append("ì–´íœ˜ ì‚¬ìš©ì´ í’ë¶€í•©ë‹ˆë‹¤!")
        
        # ë¬¸ì¥ ê¸¸ì´
        if speech['avg_sentence_length'] > 20:
            recommendations.append(f"ë¬¸ì¥ì´ ê¹ë‹ˆë‹¤ (í‰ê·  {speech['avg_sentence_length']:.1f}ë‹¨ì–´). ì§§ê³  ëª…í™•í•˜ê²Œ ë§í•´ë³´ì„¸ìš”.")
        elif speech['avg_sentence_length'] < 5:
            recommendations.append("ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ì¡°ê¸ˆ ë” ìì„¸íˆ ì„¤ëª…í•´ë³´ì„¸ìš”.")
    
    if not recommendations:
        recommendations.append("í›Œë¥­í•œ ë‹µë³€ì…ë‹ˆë‹¤!")
        if tremor:
            recommendations.append("ëª©ì†Œë¦¬ê°€ ì•ˆì •ì ì´ê³  ëª…í™•í•©ë‹ˆë‹¤.")
        if speech:
            recommendations.append("ì–¸ì–´ ì‚¬ìš©ì´ ì ì ˆí•©ë‹ˆë‹¤.")
    
    return recommendations

# ==================== ê¸°ì¡´ ìŒì„± ë¶„ì„ í•¨ìˆ˜ë“¤ ====================

def analyze_audio_chunk(audio_data, sample_rate=16000):
    """ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì²­í¬ ë¶„ì„"""
    try:
        volume = np.sqrt(np.mean(audio_data**2))
        is_speaking = volume > 0.01
        
        if len(audio_data) > 512:
            fft = np.fft.fft(audio_data)
            freqs = np.fft.fftfreq(len(fft), 1/sample_rate)
            magnitude = np.abs(fft)
            fundamental_freq = freqs[np.argmax(magnitude[1:len(magnitude)//2]) + 1] if len(magnitude) > 1 else 0
            voice_tremor = np.std(magnitude[1:100]) if len(magnitude) > 100 else 0
        else:
            fundamental_freq = 0
            voice_tremor = 0
        
        return {
            'volume': float(volume),
            'is_speaking': is_speaking,
            'fundamental_freq': float(fundamental_freq),
            'voice_tremor': float(voice_tremor),
            'timestamp': time.time()
        }
    except Exception as e:
        print(f"ìŒì„± ë¶„ì„ ì˜¤ë¥˜: {e}")
        return None

def generate_voice_analysis_report():
    """ìµœì¢… ìŒì„± ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± (AI ë¶„ì„ í¬í•¨)"""
    try:
        sample_count = len(voice_analysis_data["audio_samples"])
        
        print(f"ğŸ“Š ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘: {sample_count}ê°œ ìƒ˜í”Œ")
        
        # ê¸°ë³¸ ë¶„ì„
        if sample_count < 10:
            print(f"âš ï¸ ìƒ˜í”Œ ë¶€ì¡±: {sample_count}ê°œ")
        
        volumes = [sample['volume'] for sample in voice_analysis_data["audio_samples"] if sample]
        speaking_periods = [sample for sample in voice_analysis_data["audio_samples"] if sample and sample.get('is_speaking', False)]
        voice_tremors = [sample.get('voice_tremor', 0) for sample in speaking_periods if sample.get('voice_tremor', 0) > 0]
        
        total_time = time.time() - voice_analysis_data["start_time"] if voice_analysis_data["start_time"] else 0
        speaking_time = len(speaking_periods) * 0.1
        speaking_ratio = speaking_time / total_time if total_time > 0 else 0
        
        avg_volume = np.mean(volumes) if volumes else 0
        volume_std = np.std(volumes) if len(volumes) > 1 else 0
        volume_consistency = max(0, 1 - (volume_std / (avg_volume + 0.001)))
        
        avg_tremor = np.mean(voice_tremors) if voice_tremors else 0
        tremor_score = max(0, 1 - (avg_tremor / 0.1))
        
        confidence_score = (
            (min(avg_volume * 100, 100) * 0.30) +
            (volume_consistency * 100 * 0.25) +
            (tremor_score * 100 * 0.25) +
            (speaking_ratio * 100 * 0.20)
        )
        
        confidence_score = min(100, max(0, confidence_score))
        
        # AI ë¶„ì„ ì ìˆ˜ ë°˜ì˜
        ai_analysis = voice_analysis_data.get("ai_analysis", {})
        
        if ai_analysis:
            print("ğŸ¤– AI ë¶„ì„ ê²°ê³¼ í†µí•© ì¤‘...")
            
            # librosa ìŒì„± ì•ˆì •ì„± ì ìˆ˜
            if ai_analysis.get('tremor'):
                tremor_confidence = ai_analysis['tremor']['voice_confidence']
                confidence_score = (confidence_score * 0.5) + (tremor_confidence * 0.5)
                print(f"   - librosa ìŒì„± ì•ˆì •ì„±: {tremor_confidence}")
            
            # Whisper ì¶”ì„ìƒˆ ì ìˆ˜
            if ai_analysis.get('speech'):
                filler_penalty = min(ai_analysis['speech']['filler_ratio'], 20)
                confidence_score = confidence_score * (1 - filler_penalty / 100)
                print(f"   - ì¶”ì„ìƒˆ ë¹„ìœ¨: {ai_analysis['speech']['filler_ratio']:.1f}%")
        
        # ì¶”ì²œì‚¬í•­ ìƒì„±
        recommendations = []
        
        if avg_volume < 0.3:
            recommendations.append("ëª©ì†Œë¦¬ê°€ ì‘ìŠµë‹ˆë‹¤. ë” ìì‹ ìˆê²Œ ë§í•´ë³´ì„¸ìš”.")
        elif avg_volume > 0.8:
            recommendations.append("ëª©ì†Œë¦¬ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ì¡°ê¸ˆ ë‚®ì¶°ë³´ì„¸ìš”.")
        
        if volume_consistency < 0.7:
            recommendations.append("ëª©ì†Œë¦¬ í¬ê¸°ê°€ ì¼ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        if avg_tremor > 0.05:
            recommendations.append("ìŒì„±ì— ë–¨ë¦¼ì´ ê°ì§€ë©ë‹ˆë‹¤. ì‹¬í˜¸í¡ í›„ ì²œì²œíˆ ë§í•´ë³´ì„¸ìš”.")
        
        if speaking_ratio < 0.6:
            recommendations.append("ë§í•˜ëŠ” ì‹œê°„ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ë” ì ê·¹ì ìœ¼ë¡œ ë‹µë³€í•´ë³´ì„¸ìš”.")
        
        # AI ì¶”ì²œì‚¬í•­ ì¶”ê°€
        if ai_analysis:
            ai_recommendations = generate_ai_recommendations(
                ai_analysis.get('tremor'),
                ai_analysis.get('speech')
            )
            recommendations.extend(ai_recommendations)
        
        if not recommendations:
            recommendations.append("ì „ë°˜ì ìœ¼ë¡œ ì•ˆì •ì ì¸ ìŒì„±ìœ¼ë¡œ ë‹µë³€í•˜ì…¨ìŠµë‹ˆë‹¤!")
        
# ì•½ 550ë²ˆì§¸ ì¤„ ë¶€ê·¼
        report = {
            "overall_score": round(confidence_score, 1),
            "detailed_analysis": {
                "voice_confidence": round(confidence_score, 1),
                "average_volume": round(avg_volume * 100, 1),
                "volume_consistency": round(volume_consistency * 100, 1),
                "voice_stability": round(tremor_score * 100, 1),
                "speaking_ratio": round(speaking_ratio * 100, 1),
                "total_speaking_time": round(speaking_time, 1),
                "filler_word_count": 0,  # ê¸°ë³¸ê°’
                "filler_ratio": 0  # ê¸°ë³¸ê°’
            },
            "recommendations": recommendations[:10],
            "analysis_timestamp": time.time(),
            "ai_powered": bool(ai_analysis),
            "debug_info": {
                "samples_analyzed": sample_count,
                "chunks_received": voice_analysis_data.get("chunk_count", 0),
                "speaking_periods": len(speaking_periods),
                "has_librosa": ai_analysis.get('tremor') is not None,
                "has_whisper": ai_analysis.get('speech') is not None
            }
        }

        # ğŸ”§ AI ìƒì„¸ ë¶„ì„ ì¶”ê°€ (ì—¬ê¸°ê°€ ì¤‘ìš”!)
        if ai_analysis.get('speech'):
            speech = ai_analysis['speech']
            
            # detailed_analysisì— AI ë°ì´í„° ì¶”ê°€
            report["detailed_analysis"]["filler_word_count"] = speech['filler_total']
            report["detailed_analysis"]["filler_ratio"] = round(speech['filler_ratio'], 1)
            report["detailed_analysis"]["connective_count"] = speech['connective_total']
            report["detailed_analysis"]["vocabulary_richness"] = round(speech['vocabulary_richness'], 1)
            report["detailed_analysis"]["avg_sentence_length"] = round(speech['avg_sentence_length'], 1)
            
            # ai_detailsì— ìƒì„¸ ì •ë³´ ì¶”ê°€
            report["ai_details"] = {
                "recognized_text": speech['full_text'][:500] + "..." if len(speech['full_text']) > 500 else speech['full_text'],
                "filler_words": speech['filler_words'],
                "connectives": speech['connectives'],
                "top_nouns": speech['top_nouns']
            }

        if ai_analysis.get('tremor'):
            tremor = ai_analysis['tremor']
            report["detailed_analysis"]["jitter_percent"] = round(tremor['jitter_percent'], 2)
            report["detailed_analysis"]["shimmer"] = round(tremor['shimmer'], 4)
            report["detailed_analysis"]["average_pitch"] = round(tremor['average_pitch'], 1)

        voice_analysis_data["final_report"] = report
        voice_analysis_data["analysis_complete"] = True

        print(f"âœ… ìŒì„± ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {confidence_score:.1f}ì ")
        if ai_analysis:
            print(f"   ğŸ¤– AI ë¶„ì„ ì ìš©ë¨")
            if ai_analysis.get('speech'):
                print(f"      - ì¶”ì„ìƒˆ: {speech['filler_total']}íšŒ ({speech['filler_ratio']:.1f}%)")
                print(f"      - ì¸ì‹ëœ í…ìŠ¤íŠ¸: {len(speech['full_text'])}ì")

        return report
        
    except Exception as e:
        print(f"âŒ ìŒì„± ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return {
            "overall_score": 0,
            "detailed_analysis": {
                "voice_confidence": 0,
                "volume_consistency": 0,
                "voice_confidence": 0,
                "volume_consistency": 0,
                "voice_stability": 0,
                "speaking_ratio": 0,
                "total_speaking_time": 0,
                "filler_word_count": 0,
                "filler_ratio": 0
            },
            "recommendations": [
                f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
            ],
            "error": str(e)
        }

# ==================== ì¹´ë©”ë¼ ì´ˆê¸°í™” ====================

def initialize_camera():
    """ë‚´ì¥ ì¹´ë©”ë¼ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ íƒìƒ‰ í›„ ì´ˆê¸°í™”"""
    print("ğŸ“± ì¹´ë©”ë¼ ì´ˆê¸°í™” ì¤‘...")
    camera = None

    for camera_index in range(5):
        print(f"ğŸ” ì¹´ë©”ë¼ {camera_index}ë²ˆ ì‹œë„...")
        cap = cv2.VideoCapture(camera_index)
        if cap.isOpened():
            ret, frame = cap.read()
            if ret and frame is not None:
                print(f"âœ… ì¹´ë©”ë¼ {camera_index}ë²ˆ ì—°ê²° ì„±ê³µ")
                camera = cap
                break
        cap.release()

    if not camera:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´ë©”ë¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    camera.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
    camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
    camera.set(cv2.CAP_PROP_FPS, 30)

    print("ğŸ¥ ì¹´ë©”ë¼ ì¤€ë¹„ ì™„ë£Œ")
    return camera

camera = initialize_camera()
if camera is None:
    print("ğŸ’¥ ì¹´ë©”ë¼ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit(1)

def play_beep():
    try:
        import os
        if os.name == 'posix':
            os.system('afplay /System/Library/Sounds/Ping.aiff &')
        elif os.name == 'nt':
            import winsound
            winsound.Beep(1000, 200)
    except:
        print('\a')

# ==================== ë©´ì ‘ ì§ˆë¬¸ ìƒì„± ====================

def generate_structured_interview_questions():
    """íë¦„ì— ë§ëŠ” ì²´ê³„ì  ë©´ì ‘ ì§ˆë¬¸ ìƒì„±"""
    structured_questions = []
    
    first_question = {
        'category': 'ê¸°ë³¸ì§ˆë¬¸',
        'question': 'ê°„ë‹¨í•˜ê²Œ ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.',
        'stage': 0
    }
    structured_questions.append(first_question)
    
    for stage_idx, category in enumerate(INTERVIEW_FLOW):
        if category == "ê¸°ë³¸ì§ˆë¬¸":
            continue
        
        available_questions = INTERVIEW_QUESTIONS[category]
        selected_question = random.choice(available_questions)
        
        structured_questions.append({
            'category': category,
            'question': selected_question,
            'stage': stage_idx
        })
    
    print("ğŸ“‹ ì²´ê³„ì  ë©´ì ‘ ì§ˆë¬¸ êµ¬ì„±:")
    for i, q in enumerate(structured_questions, 1):
        print(f"   {i}. [{q['category']}] {q['question'][:40]}...")
    
    return structured_questions

# ==================== ë©´ì ‘ ìë™ ì§„í–‰ ====================

def auto_next_phase():
    global interview_session
    
    if not interview_session['active']:
        return
    
    current_phase = interview_session['phase']
    
    if current_phase == "thinking":
        print(f"ğŸ’­ ìƒê° ì‹œê°„ ì¢…ë£Œ! ë‹µë³€ ì‹œì‘ (Q{interview_session['question_index']})")
        interview_session['phase'] = "answering"
        interview_session['phase_start_time'] = time.time()
        
        interview_session['auto_timer'] = threading.Timer(
            interview_session['answer_time'], 
            auto_next_phase
        )
        interview_session['auto_timer'].start()
        
    elif current_phase == "answering":
        print(f"â° ë‹µë³€ ì‹œê°„ ì¢…ë£Œ! ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì´ë™")
        next_question_auto()

def next_question_auto():
    global interview_session
    
    if interview_session['question_index'] < len(interview_session['questions_list']):
        current_q = interview_session['questions_list'][interview_session['question_index']]
        
        interview_session.update({
            'question_index': interview_session['question_index'] + 1,
            'current_question': current_q['question'],
            'category': current_q['category'],
            'phase': 'thinking',
            'phase_start_time': time.time(),
            'current_flow_stage': current_q.get('stage', 0)
        })
        
        print(f"ğŸ“ {current_q['category']} ë‹¨ê³„: {current_q['question'][:50]}...")
        
        interview_session['auto_timer'] = threading.Timer(
            interview_session['thinking_time'], 
            auto_next_phase
        )
        interview_session['auto_timer'].start()
        
    else:
        print("ğŸ‰ ëª¨ë“  ì§ˆë¬¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        interview_session['phase'] = 'finished'
        
        voice_analysis_data["session_active"] = False
        
        # AI ì¢…í•© ë¶„ì„ ì‹¤í–‰
        threading.Thread(target=run_comprehensive_ai_analysis, daemon=True).start()
        
        threading.Timer(3.0, stop_interview_internal).start()

def run_comprehensive_ai_analysis():
    """ë©´ì ‘ ì¢…ë£Œ í›„ AI ì¢…í•© ë¶„ì„"""
    try:
        print("ğŸ¤– AI ì¢…í•© ë¶„ì„ ì‹œì‘...")
        
        # ì €ì¥ëœ ìµœì¢… ì˜¤ë””ì˜¤ íŒŒì¼ í™•ì¸
        audio_file = "final_interview_audio.webm"
        
        if not os.path.exists(audio_file):
            print(f"âš ï¸ ì˜¤ë””ì˜¤ íŒŒì¼ ì—†ìŒ: {audio_file}")
            # ê¸°ë³¸ ë¦¬í¬íŠ¸ ìƒì„±
            generate_voice_analysis_report()
            return
        
        # librosa ìŒì • ë–¨ë¦¼ ë¶„ì„
        tremor_analysis = None
        if LIBROSA_AVAILABLE:
            tremor_analysis = analyze_voice_tremor_librosa(audio_file)
            if tremor_analysis:
                voice_analysis_data["ai_analysis"]["tremor"] = tremor_analysis
        
        # Whisper + KoNLPy í…ìŠ¤íŠ¸ ë¶„ì„
        speech_analysis = None
        if WHISPER_AVAILABLE:
            speech_analysis = analyze_speech_with_whisper_konlpy(audio_file)
            if speech_analysis:
                voice_analysis_data["ai_analysis"]["speech"] = speech_analysis
        
        # ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
        generate_voice_analysis_report()
        
        print("âœ… AI ì¢…í•© ë¶„ì„ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ AI ì¢…í•© ë¶„ì„ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        # ì˜¤ë¥˜ ì‹œì—ë„ ê¸°ë³¸ ë¦¬í¬íŠ¸ëŠ” ìƒì„±
        generate_voice_analysis_report()

def stop_interview_internal():
    """ë‚´ë¶€ì ìœ¼ë¡œ ë©´ì ‘ì„ ì¤‘ì§€í•˜ëŠ” í•¨ìˆ˜"""
    global interview_session
    
    if interview_session.get('auto_timer'):
        interview_session['auto_timer'].cancel()
    
    interview_session.update({
        'active': False,
        'current_question': None,
        'phase': 'finished',
        'auto_timer': None,
        'current_flow_stage': 0
    })
    
    voice_analysis_data["session_active"] = False
    
    print("ğŸ¬ ë©´ì ‘ ìë™ ì¢…ë£Œ ì™„ë£Œ")

# ==================== ìì„¸ ë¶„ì„ê¸° ====================

class PrecisePostureAnalyzer:
    def __init__(self):
        self.guide_frame = {
            'x_min': 0.25, 'x_max': 0.75,
            'y_min': 0.15, 'y_max': 0.85
        }
        self.calibrated = False
        self.calibration_samples = []
        self.thresholds = {
            'head_rotation': 0.03, 'head_tilt': 0.02,
            'shoulder_level': 0.05, 'movement': 0.03,
            'face_touch': 0.12, 'arm_cross': 0.15, 'slouch': 0.13
        }
        self.previous_positions = deque(maxlen=15)
        self.last_beep_time = 0
        self.beep_cooldown = 2.0

    def add_calibration_sample(self, landmarks):
        nose = landmarks[mp_pose.PoseLandmark.NOSE.value]
        left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]
        right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]
        shoulder_center_y = (left_shoulder.y + right_shoulder.y) / 2
        nose_to_shoulder_y = nose.y - shoulder_center_y
        self.calibration_samples.append({
            'nose_to_shoulder_y': nose_to_shoulder_y,
            'nose_x': nose.x
        })

    def finalize_calibration(self):
        if len(self.calibration_samples) >= 10:
            avg_nose_to_shoulder_y = sum(s['nose_to_shoulder_y'] for s in self.calibration_samples) / len(self.calibration_samples)
            avg_nose_x = sum(s['nose_x'] for s in self.calibration_samples) / len(self.calibration_samples)
            self.baseline_nose_to_shoulder_y = avg_nose_to_shoulder_y
            self.baseline_nose_x = avg_nose_x
            self.calibrated = True
            print(f"âœ… ìì„¸ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì™„ë£Œ!")
            return True
        return False

    def reset_calibration(self):
        self.calibrated = False
        self.calibration_samples = []

    def calculate_distance(self, point1, point2):
        return np.sqrt((point1.x - point2.x) ** 2 + (point1.y - point2.y) ** 2)

    def check_frame_position(self, landmarks):
        global is_analysis_active
        nose = landmarks[mp_pose.PoseLandmark.NOSE.value]
        margin = 0.1
        x_min_with_margin = self.guide_frame['x_min'] + margin
        x_max_with_margin = self.guide_frame['x_max'] - margin
        y_min_with_margin = self.guide_frame['y_min'] + margin
        y_max_with_margin = self.guide_frame['y_max'] - margin

        out_of_frame = (
            nose.x < x_min_with_margin or nose.x > x_max_with_margin or
            nose.y < y_min_with_margin or nose.y > y_max_with_margin
        )

        if out_of_frame and is_analysis_active:
            current_time = time.time()
            if current_time - self.last_beep_time > self.beep_cooldown:
                threading.Thread(target=play_beep, daemon=True).start()
                self.last_beep_time = current_time

        return not out_of_frame

    def analyze_face_direction(self, landmarks):
        issues = []
        left_eye = landmarks[mp_pose.PoseLandmark.LEFT_EYE.value]
        right_eye = landmarks[mp_pose.PoseLandmark.RIGHT_EYE.value]
        nose = landmarks[mp_pose.PoseLandmark.NOSE.value]

        current_time = time.time()
        eye_distance = abs(left_eye.x - right_eye.x)
        eye_height_diff = left_eye.y - right_eye.y
        
        normalized_tilt = eye_height_diff / eye_distance if eye_distance >= 0.05 else 0
        
        self.previous_positions.append({
            'nose_x': nose.x, 'nose_y': nose.y,
            'tilt': normalized_tilt, 'timestamp': current_time
        })

        if len(self.previous_positions) >= 8:
            recent_positions = list(self.previous_positions)[-8:]
            avg_x = np.mean([p['nose_x'] for p in recent_positions])
            std_x = np.std([p['nose_x'] for p in recent_positions])
            deviation = abs(nose.x - avg_x)
            
            if deviation > 0.03 and std_x > 0.02:
                issues.append("ê³ ê°œë¥¼ ì¢Œìš°ë¡œ í”ë“¤ì§€ ë§ˆì„¸ìš”")

        if len(self.previous_positions) >= 5:
            recent_tilts = [p['tilt'] for p in list(self.previous_positions)[-5:]]
            avg_tilt = np.mean(recent_tilts)
            
            if avg_tilt > 0.25:
                issues.append("ê³ ê°œê°€ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê¸°ìš¸ì–´ì ¸ ìˆìŠµë‹ˆë‹¤")
            elif avg_tilt < -0.25:
                issues.append("ê³ ê°œê°€ ì™¼ìª½ìœ¼ë¡œ ê¸°ìš¸ì–´ì ¸ ìˆìŠµë‹ˆë‹¤")

        return issues, {'eye_tilt': float(normalized_tilt), 'face_rotation': float(nose.x)}

    def analyze_shoulder_posture(self, landmarks):
        issues = []
        left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]
        right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]
        left_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]
        right_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]

        shoulder_diff = left_shoulder.y - right_shoulder.y
        if shoulder_diff > 0.07:
            issues.append("ì™¼ìª½ ì–´ê¹¨ê°€ ì˜¬ë¼ê°€ ìˆìŠµë‹ˆë‹¤")
        elif shoulder_diff < -0.07:
            issues.append("ì˜¤ë¥¸ìª½ ì–´ê¹¨ê°€ ì˜¬ë¼ê°€ ìˆìŠµë‹ˆë‹¤")

        shoulder_center_y = (left_shoulder.y + right_shoulder.y) / 2
        hip_center_y = (left_hip.y + right_hip.y) / 2
        if shoulder_center_y > hip_center_y + 0.13:
            issues.append("ë“±ì„ í´ê³  ë°”ë¥´ê²Œ ì•‰ìœ¼ì„¸ìš”")

        return issues

    def analyze_hand_gestures(self, landmarks):
        issues = []
        nose = landmarks[mp_pose.PoseLandmark.NOSE.value]
        left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]
        right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]
        left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]
        right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]

        if self.calculate_distance(left_wrist, nose) < self.thresholds['face_touch']:
            issues.append("ì™¼ì†ì´ ì–¼êµ´ ê·¼ì²˜ì— ìˆìŠµë‹ˆë‹¤")
        if self.calculate_distance(right_wrist, nose) < self.thresholds['face_touch']:
            issues.append("ì˜¤ë¥¸ì†ì´ ì–¼êµ´ ê·¼ì²˜ì— ìˆìŠµë‹ˆë‹¤")

        wrist_distance = self.calculate_distance(left_wrist, right_wrist)
        avg_wrist_y = (left_wrist.y + right_wrist.y) / 2
        avg_shoulder_y = (left_shoulder.y + right_shoulder.y) / 2
        if wrist_distance < self.thresholds['arm_cross'] and avg_wrist_y > avg_shoulder_y:
            issues.append("íŒ”ì§±ì„ ë¼ì§€ ë§ˆì„¸ìš”")

        return issues

    def analyze_movement(self):
        if len(self.previous_positions) < 2:
            return [], 0.0

        recent_positions = list(self.previous_positions)[-5:]
        if len(recent_positions) < 2:
            return [], 0.0

        movements = []
        for i in range(1, len(recent_positions)):
            prev = recent_positions[i-1]
            curr = recent_positions[i]
            movement = np.sqrt(
                (curr['nose_x'] - prev['nose_x']) ** 2 +
                (curr['nose_y'] - prev['nose_y']) ** 2
            )
            movements.append(movement)

        avg_movement = np.mean(movements) if movements else 0.0
        issues = []
        if avg_movement > self.thresholds['movement']:
            issues.append("ëª¸ì´ ë§ì´ í”ë“¤ë¦½ë‹ˆë‹¤")

        return issues, float(avg_movement)

    def analyze_posture(self, landmarks):
        issues = []
        score = 100

        in_frame = self.check_frame_position(landmarks)
        if not in_frame:
            issues.append("ê°€ì´ë“œ ì˜ì—­ ì•ˆìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”")
            score -= 30

        face_issues, _ = self.analyze_face_direction(landmarks)
        issues.extend(face_issues)
        score -= len(face_issues) * 15

        shoulder_issues = self.analyze_shoulder_posture(landmarks)
        issues.extend(shoulder_issues)
        score -= len(shoulder_issues) * 15

        hand_issues = self.analyze_hand_gestures(landmarks)
        issues.extend(hand_issues)
        score -= len(hand_issues) * 10

        movement_issues, movement_val = self.analyze_movement()
        issues.extend(movement_issues)
        score -= len(movement_issues) * 10

        return {
            'score': max(0, score),
            'issues': issues,
            'in_frame': in_frame,
            'movement': movement_val
        }

analyzer = PrecisePostureAnalyzer()

latest_analysis_data = {
    'score': 100, 'issues': [], 'timestamp': time.time(),
    'interview': {
        'active': False, 'current_question': None,
        'phase': 'thinking', 'time_remaining': 0
    }
}

is_analysis_active = False
calibration_start_time = None
calibration_in_progress = False

# ==================== ë¹„ë””ì˜¤ ê·¸ë¦¬ê¸° í•¨ìˆ˜ë“¤ ====================

def draw_fixed_guide_frame(frame, h, w):
    guide = analyzer.guide_frame
    x1, y1 = int(w * guide['x_min']), int(h * guide['y_min'])
    x2, y2 = int(w * guide['x_max']), int(h * guide['y_max'])

    overlay = frame.copy()
    cv2.rectangle(overlay, (0, 0), (w, y1), (0, 0, 0), -1)
    cv2.rectangle(overlay, (0, y2), (w, h), (0, 0, 0), -1)
    cv2.rectangle(overlay, (0, y1), (x1, y2), (0, 0, 0), -1)
    cv2.rectangle(overlay, (x2, y1), (w, y2), (0, 0, 0), -1)
    cv2.addWeighted(overlay, 0.3, frame, 0.7, 0, frame)

    color = (0, 255, 0)
    thickness = 3
    cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness)



def generate_frames():
    global latest_analysis_data, is_analysis_active, calibration_start_time, calibration_in_progress

    while True:
        success, frame = camera.read()
        if not success:
            print("âš ï¸ ì¹´ë©”ë¼ì—ì„œ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break

        frame = cv2.flip(frame, 1)
        h, w, _ = frame.shape

        draw_fixed_guide_frame(frame, h, w)


        if pose:
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pose_results = pose.process(rgb_frame)

            if pose_results.pose_landmarks:
                try:
                    if calibration_in_progress:
                        elapsed = time.time() - calibration_start_time
                        if elapsed < 5.0:
                            analyzer.add_calibration_sample(pose_results.pose_landmarks.landmark)
                            countdown = 5 - int(elapsed)
                            cv2.putText(frame, f"Calibrating... {countdown}", (w // 2 - 150, h // 2),
                                        cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 255), 3)
                            cv2.putText(frame, "Keep current posture", (w // 2 - 150, h // 2 + 50),
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
                        else:
                            analyzer.finalize_calibration()
                            calibration_in_progress = False
                            is_analysis_active = True

                    elif analyzer.calibrated and is_analysis_active:
                        analysis = analyzer.analyze_posture(pose_results.pose_landmarks.landmark)

                        current_time = time.time()
                        time_remaining = 0
                        if interview_session['phase_start_time']:
                            elapsed = int(current_time - interview_session['phase_start_time'])
                            if interview_session['phase'] == 'thinking':
                                time_remaining = max(0, interview_session['thinking_time'] - elapsed)
                            elif interview_session['phase'] == 'answering':
                                time_remaining = max(0, interview_session['answer_time'] - elapsed)

                        interview_info = {
                            'active': interview_session["active"],
                            'current_question': interview_session["current_question"],
                            'category': interview_session["category"],
                            'question_number': interview_session["question_index"],
                            'total_questions': len(interview_session["questions_list"]),
                            'phase': interview_session["phase"],
                            'time_remaining': time_remaining,
                            'current_stage': INTERVIEW_FLOW[interview_session.get('current_flow_stage', 0)]
                        }

                        latest_analysis_data = {
                            'score': analysis['score'],
                            'issues': analysis['issues'],
                            'timestamp': time.time(),
                            'interview': interview_info
                        }

                        if not interview_session["active"]:
                            y_offset = 30
                            score_color = (0, 255, 0) if analysis['score'] >= 80 else (0, 165, 255) if analysis['score'] >= 60 else (0, 0, 255)
                            cv2.putText(frame, f"Score: {analysis['score']}", (10, y_offset),
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, score_color, 2)
                            y_offset += 35

                            if analysis['issues']:
                                for issue in analysis['issues']:
                                    cv2.putText(frame, issue, (10, y_offset),
                                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
                                    y_offset += 25
                            else:
                                cv2.putText(frame, "Perfect!", (10, y_offset),
                                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                    else:
                        cv2.putText(frame, "Waiting for calibration...", (10, 30),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)

                except Exception as e:
                    print(f"ë¶„ì„ ì˜¤ë¥˜: {e}")
            else:
                cv2.putText(frame, "No pose detected", (10, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

        ret, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
        if not ret:
            continue

        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')

def generate_data_stream():
    while True:
        current_time = time.time()
        time_remaining = 0
        
        if interview_session['phase_start_time']:
            elapsed = int(current_time - interview_session['phase_start_time'])
            if interview_session['phase'] == 'thinking':
                time_remaining = max(0, interview_session['thinking_time'] - elapsed)
            elif interview_session['phase'] == 'answering':
                time_remaining = max(0, interview_session['answer_time'] - elapsed)

        interview_info = {
            'active': interview_session['active'],
            'current_question': interview_session['current_question'],
            'category': interview_session['category'],
            'question_number': interview_session['question_index'],
            'total_questions': len(interview_session['questions_list']),
            'phase': interview_session['phase'],
            'time_remaining': time_remaining,
            'current_stage': INTERVIEW_FLOW[interview_session.get('current_flow_stage', 0)] if interview_session['active'] else None
        }

        voice_info = {
            'session_active': voice_analysis_data["session_active"],
            'analysis_complete': voice_analysis_data["analysis_complete"],
            'final_report': voice_analysis_data["final_report"] if voice_analysis_data["analysis_complete"] else None,
            'has_data': len(voice_analysis_data["audio_samples"]) > 0,
            'sample_count': len(voice_analysis_data["audio_samples"]),
            'filler_count': len(voice_analysis_data["filler_words"]),
            'chunk_count': voice_analysis_data["chunk_count"],
            'ai_enabled': WHISPER_AVAILABLE and LIBROSA_AVAILABLE
        }

        stream_data = {
            **latest_analysis_data,
            'interview': interview_info,
            'voice_analysis': voice_info
        }

        yield f"data: {json.dumps(stream_data)}\n\n"
        time.sleep(0.5)

# ==================== API ì—”ë“œí¬ì¸íŠ¸ë“¤ ====================

@app.route('/health')
def health_check():
    return jsonify({
        'status': 'ok', 
        'camera': camera.isOpened() if camera else False,
        'voice_analysis': 'AI-Powered' if (WHISPER_AVAILABLE and LIBROSA_AVAILABLE) else 'Basic',
        'ai_modules': {
            'whisper': WHISPER_AVAILABLE,
            'konlpy': KONLPY_AVAILABLE,
            'librosa': LIBROSA_AVAILABLE,
            'pydub': AUDIO_PROCESSING_AVAILABLE
        },
        'resolution': f"{int(camera.get(cv2.CAP_PROP_FRAME_WIDTH))}x{int(camera.get(cv2.CAP_PROP_FRAME_HEIGHT))}" if camera else "N/A"
    })

@app.route('/video_feed')
def video_feed():
    return Response(generate_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/stream_data')
def stream_data():
    return Response(generate_data_stream(), mimetype='text/event-stream',
                    headers={'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'})

@app.route('/start_analysis', methods=['POST'])
def start_analysis():
    global is_analysis_active, calibration_start_time, calibration_in_progress
    analyzer.reset_calibration()
    calibration_start_time = time.time()
    calibration_in_progress = True
    is_analysis_active = False
    print("ğŸ¯ ìì„¸ ë¶„ì„ ì‹œì‘ - ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì¤‘...")
    return jsonify({'status': 'calibration_started'})

@app.route('/stop_analysis', methods=['POST'])
def stop_analysis():
    global is_analysis_active, calibration_in_progress
    is_analysis_active = False
    calibration_in_progress = False
    analyzer.last_beep_time = 0
    
    if interview_session.get('auto_timer'):
        interview_session['auto_timer'].cancel()
    
    interview_session.update({
        'active': False, 
        'current_question': None,
        'phase': 'finished', 
        'auto_timer': None
    })
    
    # ìŒì„± ë¶„ì„ ë°ì´í„° ì´ˆê¸°í™”
    voice_analysis_data["session_active"] = False
    voice_analysis_data["analysis_complete"] = False
    voice_analysis_data["final_report"] = {}
    voice_analysis_data["ai_analysis"] = {}
    
    print("ğŸ›‘ ìì„¸ ë¶„ì„ ì¤‘ì§€")
    return jsonify({'status': 'stopped'})

@app.route('/interview/start', methods=['POST'])
def start_interview():
    global interview_session
    
    if interview_session.get('auto_timer'):
        interview_session['auto_timer'].cancel()
    
    structured_questions = generate_structured_interview_questions()
    first_q = structured_questions[0]
    
    interview_session.update({
        'active': True,
        'questions_list': structured_questions,
        'question_index': 1,
        'current_question': first_q['question'],
        'category': first_q['category'],
        'phase': 'thinking',
        'phase_start_time': time.time(),
        'current_flow_stage': first_q['stage']
    })
    
    # ìŒì„± ë¶„ì„ ë°ì´í„° ì™„ì „ ì´ˆê¸°í™”
    voice_analysis_data.clear()
    voice_analysis_data.update({
        "session_active": True,
        "start_time": time.time(),
        "audio_samples": [],
        "filler_words": [],
        "voice_confidence": [],
        "speaking_pace": [],
        "volume_levels": [],
        "total_speaking_time": 0,
        "silence_periods": [],
        "analysis_complete": False,
        "final_report": {},
        "chunk_count": 0,
        "ai_analysis": {}
    })
    
    print(f"ğŸ¬ ì²´ê³„ì  ë©´ì ‘ ì‹œì‘! ì´ {len(structured_questions)}ê°œ ì§ˆë¬¸")
    print(f"ğŸ¤ AI ìŒì„± ë¶„ì„ ì„¸ì…˜ í™œì„±í™” (Whisper: {WHISPER_AVAILABLE}, librosa: {LIBROSA_AVAILABLE})")
    
    interview_session['auto_timer'] = threading.Timer(interview_session['thinking_time'], auto_next_phase)
    interview_session['auto_timer'].start()
    
    return jsonify({
        'status': 'started',
        'total_questions': len(structured_questions),
        'current_question': first_q['question'],
        'category': first_q['category'],
        'flow_stage': INTERVIEW_FLOW[first_q['stage']],
        'voice_analysis_active': True,
        'ai_enabled': WHISPER_AVAILABLE and LIBROSA_AVAILABLE
    })

@app.route('/interview/stop', methods=['POST'])
def stop_interview():
    """HTTP ì—”ë“œí¬ì¸íŠ¸ë¡œì„œì˜ ë©´ì ‘ ì¤‘ì§€ í•¨ìˆ˜"""
    stop_interview_internal()
    return jsonify({'status': 'stopped'})

# MediaRecorder ì˜¤ë””ì˜¤ ì²­í¬ ìˆ˜ì‹ 
@app.route('/voice/audio_chunk_blob', methods=['POST'])
def receive_audio_chunk_blob():
    """MediaRecorderì—ì„œ ìƒì„±ëœ ì˜¤ë””ì˜¤ ì²­í¬ ìˆ˜ì‹  ë° ë¶„ì„"""
    try:
        if not voice_analysis_data["session_active"]:
            return jsonify({'status': 'session_inactive'})
        
        if 'audio' not in request.files:
            return jsonify({'error': 'No audio file'})
        
        audio_file = request.files['audio']
        chunk_number = int(request.form.get('chunk_number', 0))
        
        audio_data = audio_file.read()
        voice_analysis_data["chunk_count"] += 1
        
        # ì˜¤ë””ì˜¤ ì²­í¬ ì €ì¥ (ë‚˜ì¤‘ì— ë³‘í•©)
        chunk_path = f"temp_chunk_{chunk_number}.webm"
        with open(chunk_path, 'wb') as f:
            f.write(audio_data)
        
        print(f"ğŸ“¥ ì²­í¬ #{chunk_number} ìˆ˜ì‹ : {len(audio_data)} bytes")
        
        # ê¸°ë³¸ ë¶„ì„
        if len(audio_data) > 100:
            basic_volume = min(100, (len(audio_data) / 1000) * 8)
            
            basic_sample = {
                'volume': basic_volume / 100,
                'is_speaking': True,
                'fundamental_freq': 0,
                'voice_tremor': 0,
                'timestamp': time.time(),
                'analysis_type': 'basic'
            }
            
            voice_analysis_data["audio_samples"].append(basic_sample)
        
        # pydubë¡œ ì •ë°€ ë¶„ì„ ì‹œë„
        if AUDIO_PROCESSING_AVAILABLE:
            try:
                audio_segment = AudioSegment.from_file(
                    io.BytesIO(audio_data),
                    format="webm"
                )
                
                samples = np.array(audio_segment.get_array_of_samples(), dtype=np.float32)
                
                if audio_segment.channels == 2:
                    samples = samples.reshape((-1, 2)).mean(axis=1)
                
                if len(samples) > 0 and np.max(np.abs(samples)) > 0:
                    samples = samples / np.max(np.abs(samples))
                    
                    precise_analysis = analyze_audio_chunk(samples, audio_segment.frame_rate)
                    
                    if precise_analysis:
                        voice_analysis_data["audio_samples"][-1] = precise_analysis
                        voice_analysis_data["audio_samples"][-1]['analysis_type'] = 'precise'
                
            except Exception as e:
                print(f"âš ï¸ ì •ë°€ ë¶„ì„ ì‹¤íŒ¨ (ê¸°ë³¸ ë¶„ì„ ìœ ì§€): {e}")
        
        return jsonify({
            'status': 'success',
            'chunk_number': chunk_number,
            'samples_total': len(voice_analysis_data["audio_samples"]),
            'chunks_received': voice_analysis_data["chunk_count"]
        })
        
    except Exception as e:
        print(f"âŒ ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)})

# ìµœì¢… ì˜¤ë””ì˜¤ íŒŒì¼ ìˆ˜ì‹ 
@app.route('/voice/final_audio', methods=['POST'])
def receive_final_audio():
    """ìµœì¢… ì™„ì„±ëœ ì˜¤ë””ì˜¤ íŒŒì¼ ìˆ˜ì‹ """
    try:
        if 'final_audio' not in request.files:
            return jsonify({'error': 'No final audio file'})
        
        audio_file = request.files['final_audio']
        timestamp = request.form.get('timestamp', time.time() * 1000)
        
        # ìµœì¢… ì˜¤ë””ì˜¤ ì €ì¥
        final_audio_path = "final_interview_audio.webm"
        audio_file.save(final_audio_path)
        
        file_size = os.path.getsize(final_audio_path)
        print(f"ğŸ“¥ ìµœì¢… ì˜¤ë””ì˜¤ íŒŒì¼ ìˆ˜ì‹  - í¬ê¸°: {file_size} bytes")
        
        # AI ë¶„ì„ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)
        if WHISPER_AVAILABLE or LIBROSA_AVAILABLE:
            print("ğŸ¤– AI ë¶„ì„ ìŠ¤ë ˆë“œ ì‹œì‘...")
            threading.Thread(target=run_comprehensive_ai_analysis, daemon=True).start()
        
        return jsonify({
            'status': 'final_audio_received',
            'file_size': file_size,
            'timestamp': timestamp,
            'ai_analysis_started': WHISPER_AVAILABLE or LIBROSA_AVAILABLE
        })
        
    except Exception as e:
        print(f"âŒ ìµœì¢… ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return jsonify({'error': str(e)})

# ìŒì„± ì¸ì‹ í…ìŠ¤íŠ¸ ìˆ˜ì‹ 
@app.route('/voice/speech_text', methods=['POST'])
def receive_speech_text():
    """ìŒì„± ì¸ì‹ í…ìŠ¤íŠ¸ ìˆ˜ì‹  ë° ë¶„ì„"""
    try:
        if not voice_analysis_data["session_active"]:
            return jsonify({'status': 'session_inactive'})
        
        data = request.get_json()
        if not data or 'text' not in data:
            return jsonify({'error': 'No text data'})
        
        text = data['text']
        
        # ì¶”ì„ìƒˆ ë‹¨ì–´ ê¸°ë¡
        words = text.split()
        for word in words:
            clean_word = word.strip('.,!?').lower()
            if clean_word in FILLER_WORDS:
                voice_analysis_data["filler_words"].append(clean_word)
        
        print(f"ğŸ“ ìŒì„± í…ìŠ¤íŠ¸ ë¶„ì„: {text[:50]}...")
        
        return jsonify({
            'status': 'processed',
            'total_fillers': len(voice_analysis_data["filler_words"])
        })
        
    except Exception as e:
        print(f"ìŒì„± í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return jsonify({'error': str(e)})

@app.route('/voice/analysis_report', methods=['GET'])
def get_voice_analysis_report():
    """ìŒì„± ë¶„ì„ ë¦¬í¬íŠ¸ ì¡°íšŒ"""
    if voice_analysis_data["analysis_complete"]:
        return jsonify(voice_analysis_data["final_report"])
    else:
        return jsonify({
            'status': 'analysis_in_progress',
            'ai_enabled': WHISPER_AVAILABLE and LIBROSA_AVAILABLE,
            'samples': len(voice_analysis_data["audio_samples"])
        })

# ==================== ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ====================

def safe_close_mediapipe():
    """MediaPipe ë¦¬ì†ŒìŠ¤ë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ"""
    global pose
    
    if pose is None:
        return
    
    try:
        if hasattr(pose, '_graph'):
            if pose._graph is not None:
                pose.close()
                print("âœ… MediaPipe ì •ìƒ ì¢…ë£Œ")
            else:
                print("â„¹ï¸ MediaPipe ì´ë¯¸ ì¢…ë£Œë¨")
        else:
            try:
                pose.close()
                print("âœ… MediaPipe ì •ìƒ ì¢…ë£Œ (êµ¬ë²„ì „)")
            except:
                print("â„¹ï¸ MediaPipe ì¢…ë£Œ ì‹œë„ ì™„ë£Œ")
                
    except ValueError as e:
        if "already None" in str(e) or "Closing" in str(e):
            print("â„¹ï¸ MediaPipe ì´ë¯¸ ì¢…ë£Œëœ ìƒíƒœ")
        else:
            print(f"âš ï¸ MediaPipe ì¢…ë£Œ ì¤‘ ValueError: {e}")
    except Exception as e:
        print(f"âŒ MediaPipe ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
    finally:
        pose = None

def cleanup_resources():
    """í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ëª¨ë“  ë¦¬ì†ŒìŠ¤ë¥¼ ì•ˆì „í•˜ê²Œ ì •ë¦¬"""
    print("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...")
    
    global camera
    
    # ì¹´ë©”ë¼ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    try:
        if camera is not None:
            if camera.isOpened():
                camera.release()
                print("âœ… ì¹´ë©”ë¼ ë¦¬ì†ŒìŠ¤ í•´ì œ ì™„ë£Œ")
            camera = None
    except Exception as e:
        print(f"âŒ ì¹´ë©”ë¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # MediaPipe ì•ˆì „ ì¢…ë£Œ
    safe_close_mediapipe()
    
    # ë©´ì ‘ ì„¸ì…˜ ì •ë¦¬
    try:
        global interview_session
        if interview_session.get('auto_timer'):
            interview_session['auto_timer'].cancel()
            print("âœ… ë©´ì ‘ íƒ€ì´ë¨¸ ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ë©´ì ‘ ì„¸ì…˜ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ìŒì„± ë¶„ì„ ë°ì´í„° ì •ë¦¬
    try:
        global voice_analysis_data
        voice_analysis_data["session_active"] = False
        voice_analysis_data["audio_samples"].clear()
        voice_analysis_data["filler_words"].clear()
        print("âœ… ìŒì„± ë¶„ì„ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ìŒì„± ë¶„ì„ ë°ì´í„° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ ì •ë¦¬
    try:
        for f in os.listdir('.'):
            if f.startswith('temp_chunk_') or f == 'final_interview_audio.webm':
                os.remove(f)
                print(f"âœ… ì„ì‹œ íŒŒì¼ ì‚­ì œ: {f}")
    except Exception as e:
        print(f"âš ï¸ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    print("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

def safe_shutdown():
    """í”„ë¡œê·¸ë¨ ì•ˆì „ ì¢…ë£Œ"""
    print("\nğŸ›‘ ì•ˆì „í•œ ì„œë²„ ì¢…ë£Œ ì‹œì‘...")
    
    try:
        global interview_session
        if interview_session.get('active'):
            print("ğŸ“ ì§„í–‰ ì¤‘ì¸ ë©´ì ‘ ì¢…ë£Œ ì¤‘...")
            stop_interview_internal()
    except Exception as e:
        print(f"âŒ ë©´ì ‘ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    try:
        cleanup_resources()
    except Exception as e:
        print(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    print("âœ… ì„œë²„ ì¢…ë£Œ ì™„ë£Œ")

# atexit ë“±ë¡
atexit.register(cleanup_resources)

# ==================== ë©”ì¸ ì‹¤í–‰ ====================

if __name__ == '__main__':
    print("=" * 90)
    print("ğŸ“ AI ê¸°ë°˜ ì²´ìœ¡ëŒ€í•™ ë©´ì ‘ ë¶„ì„ ì‹œìŠ¤í…œ (Whisper + KoNLPy + librosa)")
    print("=" * 90)
    print(f"ğŸŒ ì„œë²„ ì£¼ì†Œ: http://0.0.0.0:5001")
    print(f"ğŸ“± ì¹´ë©”ë¼: {int(camera.get(cv2.CAP_PROP_FRAME_WIDTH))}x{int(camera.get(cv2.CAP_PROP_FRAME_HEIGHT))}" if camera else "N/A")
    print("ğŸ¯ ë©´ì ‘ ì‹œìŠ¤í…œ: ì²´ê³„ì  íë¦„ (ìê¸°ì†Œê°œ â†’ ì§€ì›ë™ê¸° â†’ ì „ê³µì§€ì‹ â†’ ìƒí™©ëŒ€ì²˜ â†’ ì••ë°•ì§ˆë¬¸ â†’ ë§ˆë¬´ë¦¬)")
    print("\nğŸ¤– AI ëª¨ë“ˆ ìƒíƒœ:")
    print(f"   - Whisper (ìŒì„±ì¸ì‹): {'âœ… í™œì„±í™”' if WHISPER_AVAILABLE else 'âŒ ë¹„í™œì„±í™”'}")
    print(f"   - KoNLPy (í’ˆì‚¬ë¶„ì„): {'âœ… í™œì„±í™”' if KONLPY_AVAILABLE else 'âŒ ë¹„í™œì„±í™”'}")
    print(f"   - librosa (ìŒì •ë–¨ë¦¼): {'âœ… í™œì„±í™”' if LIBROSA_AVAILABLE else 'âŒ ë¹„í™œì„±í™”'}")
    print(f"   - pydub (ì˜¤ë””ì˜¤ì²˜ë¦¬): {'âœ… í™œì„±í™”' if AUDIO_PROCESSING_AVAILABLE else 'âŒ ë¹„í™œì„±í™”'}")
    
    if WHISPER_AVAILABLE and LIBROSA_AVAILABLE:
        print("\nğŸ“Š AI ë¶„ì„ í•­ëª©:")
        print("   - ìŒì • ë–¨ë¦¼ (Jitter/Shimmer)")
        print("   - ì¶”ì„ìƒˆ ì‚¬ìš© ë¹ˆë„ ë° ì¢…ë¥˜")
        print("   - ì ‘ì†ì‚¬ ì‚¬ìš© íŒ¨í„´")
        print("   - ë°˜ë³µ ë‹¨ì–´ ê°ì§€")
        print("   - í’ˆì‚¬ë³„ ë¶„ì„ (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬)")
        print("   - ì–´íœ˜ ë‹¤ì–‘ì„±")
        print("   - ë¬¸ì¥ ê¸¸ì´ ë¶„ì„")
    else:
        print("\nâš ï¸ AI ëª¨ë“ˆ ë¯¸ì„¤ì¹˜:")
        if not WHISPER_AVAILABLE:
            print("   pip install openai-whisper")
        if not KONLPY_AVAILABLE:
            print("   pip install konlpy")
        if not LIBROSA_AVAILABLE:
            print("   pip install librosa soundfile")
    
    print("=" * 90)

    try:
        app.run(host='0.0.0.0', port=5001, debug=True, threaded=True, use_reloader=False)
    except KeyboardInterrupt:
        print("\nğŸ›‘ Ctrl+C ê°ì§€ - ì„œë²„ ì¢…ë£Œ ì¤‘...")
        safe_shutdown()
    except Exception as e:
        print(f"\nğŸ’¥ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        safe_shutdown()
    finally:
        try:
            safe_shutdown()
        except:
            pass
        print("ğŸ í”„ë¡œê·¸ë¨ ì™„ì „ ì¢…ë£Œ")